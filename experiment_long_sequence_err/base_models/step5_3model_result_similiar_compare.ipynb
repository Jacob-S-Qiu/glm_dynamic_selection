{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct a preliminary analysis of all model predictions\n",
    "1. Find the problem where all the models are correct\n",
    "2. Find the problem where all models get it wrong\n",
    "3. Find questions that get some parts right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hy_c10_train_analysis_path = \"../02-train&valid/hyena/result/hyena_c10_690_train.pt\"\n",
    "hy_c10_valid_analysis_path = \"../02-train&valid/hyena/result/hyena_c10_690_valid.pt\"\n",
    "hy_c10_test_analysis_path = \"../02-train&valid/hyena/result/hyena_c10_690_test.pt\"\n",
    "nt_c10_train_analysis_path = \"../02-train&valid/ntv2/result/ntv2_c10_train_results.pt\"\n",
    "nt_c10_valid_analysis_path = \"../02-train&valid/ntv2/result/ntv2_c10_valid_results.pt\"\n",
    "nt_c10_test_analysis_path = \"../02-train&valid/ntv2/result/ntv2_c10_test_results.pt\"\n",
    "cd_c10_train_analysis_path = \"../02-train&valid/cdgpt/result/cdgpt_c10_2_3036_train_results.pt\"\n",
    "cd_c10_valid_analysis_path = \"../02-train&valid/cdgpt/result/cdgpt_c10_2_3036_valid_results.pt\"\n",
    "cd_c10_test_analysis_path = \"../02-train&valid/cdgpt/result/cdgpt_c10_2_3036_test_results.pt\"\n",
    "\n",
    "hy_train_file_path = \"../01-data/C10_hyena_20kbp_train_dataset.pt\"\n",
    "hy_valid_file_path = \"../01-data/C10_hyena_20kbp_valid_dataset.pt\"\n",
    "hy_test_file_path = \"../01-data/C10_hyena_20kbp_test_dataset.pt\"\n",
    "\n",
    "nt_train_file_path = \"../01-data/C10_ntv2_12kbp_train_dataset.pt\"\n",
    "nt_valid_file_path = \"../01-data/C10_ntv2_12kbp_valid_dataset.pt\"\n",
    "nt_test_file_path = \"../01-data/C10_ntv2_12kbp_test_dataset.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "def compare_predictions(result_files):\n",
    "    results = [torch.load(f) for f in result_files]\n",
    "    num_samples = len(results[0][\"prediction\"])  \n",
    "    num_models = len(results)  \n",
    "    \n",
    "    # Check the predictions for each sample\n",
    "    all_correct_indices = []\n",
    "    all_incorrect_indices = []\n",
    "    one_correct_rest_incorrect = []\n",
    "    one_incorrect_rest_correct = []\n",
    "    \n",
    "    # A single model is correct and other models are wrong\n",
    "    single_model_correct_rest_incorrect = {f\"{chr(97 + i)}_correct_rest_incorrect\": [] for i in range(num_models)}\n",
    "    \n",
    "    # When a single model is wrong and other models are correct\n",
    "    single_model_incorrect_rest_correct = {f\"{chr(97 + i)}_incorrect_rest_correct\": [] for i in range(num_models)}\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        label = results[0][\"label\"][i]\n",
    "        predictions = [result[\"prediction\"][i] for result in results]\n",
    "        correct_preds = [pred == label for pred in predictions]\n",
    "        num_correct = sum(correct_preds)\n",
    "\n",
    "        if num_correct == num_models:\n",
    "            all_correct_indices.append(i)\n",
    "        \n",
    "        elif num_correct == 0:\n",
    "            all_incorrect_indices.append(i)\n",
    "        \n",
    "        elif num_correct == 1:\n",
    "            one_correct_rest_incorrect.append(i)\n",
    "            for model_idx, is_correct in enumerate(correct_preds):\n",
    "                if is_correct:\n",
    "                    model_label = chr(97 + model_idx)\n",
    "                    single_model_correct_rest_incorrect[f\"{model_label}_correct_rest_incorrect\"].append(i)\n",
    "        \n",
    "        elif num_correct == num_models - 1:\n",
    "            one_incorrect_rest_correct.append(i)\n",
    "            for model_idx, is_correct in enumerate(correct_preds):\n",
    "                if not is_correct:\n",
    "                    model_label = chr(97 + model_idx)\n",
    "                    single_model_incorrect_rest_correct[f\"{model_label}_incorrect_rest_correct\"].append(i)\n",
    "\n",
    "    def format_output(label, indices):\n",
    "        count = len(indices)\n",
    "        proportion = count / num_samples * 100\n",
    "        print(f\"{label} Quantity: {count} ({proportion:.2f}%), subscript: {indices}\")\n",
    "\n",
    "    print(\"\\n=== The model predicts the comparison results ===\")\n",
    "    format_output(\"All models predict the correct example\", all_correct_indices)\n",
    "    format_output(\"All models predict examples of errors\", all_incorrect_indices)\n",
    "    format_output(\"An example of one correct prediction and the rest wrong\", one_correct_rest_incorrect)\n",
    "    format_output(\"An example of one incorrect prediction and the rest correct\", one_incorrect_rest_correct)\n",
    "\n",
    "    print(\"\\n=== Details of how a single model predicted correctly and other models predicted incorrectly ===\")\n",
    "    for model_label, indices in single_model_correct_rest_incorrect.items():\n",
    "        format_output(f\"Example of model {model_label[0].upper()} getting it right and other models getting it wrong\", indices)\n",
    "\n",
    "    print(\"\\n=== Individual models predicted wrong and other models predicted correct details ===\")\n",
    "    for model_label, indices in single_model_incorrect_rest_correct.items():\n",
    "        format_output(f\"Example of model {model_label[0].upper()} getting it wrong and other models getting it right\", indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== The model predicts the comparison results ===\n",
      "All models predict the correct example Quantity: 474 (47.98%), subscript: [2, 3, 4, 6, 8, 10, 11, 13, 14, 15, 16, 19, 20, 22, 23, 24, 27, 28, 31, 35, 36, 39, 44, 49, 50, 52, 54, 59, 60, 63, 64, 65, 66, 67, 69, 71, 73, 76, 77, 80, 81, 83, 86, 89, 91, 93, 95, 97, 101, 103, 104, 107, 108, 112, 113, 114, 115, 123, 124, 125, 126, 129, 133, 134, 135, 139, 143, 146, 147, 148, 149, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 164, 167, 168, 169, 170, 176, 178, 179, 180, 183, 185, 187, 188, 190, 191, 197, 200, 201, 206, 207, 208, 209, 210, 212, 213, 215, 221, 222, 224, 226, 228, 236, 237, 239, 242, 243, 244, 245, 246, 247, 250, 251, 252, 253, 256, 260, 263, 266, 268, 269, 273, 274, 275, 276, 277, 278, 280, 282, 284, 285, 286, 287, 291, 293, 294, 295, 296, 297, 300, 303, 306, 309, 310, 314, 315, 320, 321, 322, 324, 325, 329, 330, 331, 332, 334, 336, 337, 338, 342, 343, 344, 346, 347, 348, 351, 357, 358, 361, 363, 365, 366, 370, 374, 376, 380, 383, 386, 388, 389, 390, 395, 397, 398, 401, 402, 403, 405, 406, 412, 414, 420, 421, 423, 424, 426, 428, 431, 433, 434, 438, 439, 440, 441, 443, 445, 446, 450, 455, 457, 460, 463, 465, 466, 468, 470, 471, 472, 475, 476, 479, 480, 483, 484, 487, 490, 491, 494, 495, 496, 510, 513, 514, 515, 516, 517, 519, 522, 526, 528, 529, 530, 533, 534, 536, 537, 539, 542, 545, 550, 552, 553, 558, 560, 561, 562, 563, 565, 568, 569, 573, 575, 579, 580, 581, 583, 585, 591, 594, 596, 597, 599, 600, 601, 602, 608, 611, 614, 615, 616, 617, 621, 622, 623, 624, 626, 627, 628, 629, 631, 635, 637, 639, 641, 642, 644, 645, 646, 648, 650, 653, 659, 660, 662, 665, 670, 671, 674, 677, 687, 689, 690, 693, 694, 697, 698, 702, 703, 705, 706, 707, 708, 712, 713, 715, 716, 719, 720, 721, 722, 724, 725, 727, 730, 731, 732, 733, 734, 739, 741, 743, 747, 748, 749, 750, 754, 755, 760, 762, 763, 765, 768, 769, 771, 772, 774, 777, 778, 782, 783, 784, 786, 788, 789, 790, 791, 792, 795, 796, 800, 802, 804, 805, 806, 809, 810, 811, 812, 813, 814, 816, 818, 822, 823, 824, 825, 826, 827, 828, 835, 836, 837, 838, 841, 842, 850, 851, 852, 854, 855, 857, 858, 861, 864, 866, 867, 868, 870, 874, 876, 878, 880, 882, 883, 885, 889, 891, 892, 893, 894, 897, 899, 900, 901, 904, 906, 909, 910, 917, 918, 920, 921, 922, 929, 931, 932, 933, 934, 938, 939, 942, 947, 948, 952, 953, 954, 955, 958, 961, 964, 965, 967, 968, 969, 973, 974, 976, 977, 980, 981, 984, 986, 987]\n",
      "All models predict examples of errors Quantity: 93 (9.41%), subscript: [0, 12, 17, 18, 34, 38, 43, 45, 70, 72, 75, 78, 87, 98, 132, 137, 150, 199, 211, 214, 219, 241, 248, 259, 279, 289, 298, 302, 305, 349, 350, 354, 362, 367, 369, 391, 408, 411, 429, 435, 447, 449, 459, 473, 481, 500, 525, 531, 546, 548, 551, 559, 564, 566, 570, 577, 578, 586, 590, 592, 606, 607, 610, 619, 633, 634, 636, 640, 649, 658, 673, 678, 679, 695, 701, 704, 717, 723, 737, 738, 740, 793, 869, 898, 903, 907, 925, 936, 945, 950, 971, 979, 982]\n",
      "An example of one correct prediction and the rest wrong Quantity: 154 (15.59%), subscript: [1, 25, 26, 29, 32, 51, 58, 61, 62, 84, 88, 92, 96, 102, 106, 118, 120, 121, 122, 127, 136, 141, 145, 172, 174, 184, 196, 218, 234, 238, 257, 258, 261, 264, 265, 267, 271, 281, 299, 301, 307, 311, 313, 317, 323, 326, 341, 345, 352, 353, 356, 359, 368, 371, 372, 377, 379, 385, 387, 392, 396, 400, 404, 413, 415, 416, 418, 432, 442, 452, 453, 456, 469, 474, 488, 489, 492, 497, 503, 504, 505, 507, 524, 540, 541, 555, 556, 572, 588, 593, 595, 605, 612, 618, 638, 652, 654, 656, 663, 666, 676, 692, 696, 700, 709, 710, 714, 729, 744, 758, 766, 770, 775, 780, 781, 785, 787, 798, 799, 803, 817, 819, 820, 829, 831, 834, 840, 844, 846, 847, 848, 856, 859, 860, 862, 871, 877, 887, 888, 890, 905, 912, 914, 916, 924, 927, 928, 944, 956, 959, 960, 962, 966, 985]\n",
      "An example of one incorrect prediction and the rest correct Quantity: 267 (27.02%), subscript: [5, 7, 9, 21, 30, 33, 37, 40, 41, 42, 46, 47, 48, 53, 55, 56, 57, 68, 74, 79, 82, 85, 90, 94, 99, 100, 105, 109, 110, 111, 116, 117, 119, 128, 130, 131, 138, 140, 142, 144, 160, 163, 165, 166, 171, 173, 175, 177, 181, 182, 186, 189, 192, 193, 194, 195, 198, 202, 203, 204, 205, 216, 217, 220, 223, 225, 227, 229, 230, 231, 232, 233, 235, 240, 249, 254, 255, 262, 270, 272, 283, 288, 290, 292, 304, 308, 312, 316, 318, 319, 327, 328, 333, 335, 339, 340, 355, 360, 364, 373, 375, 378, 381, 382, 384, 393, 394, 399, 407, 409, 410, 417, 419, 422, 425, 427, 430, 436, 437, 444, 448, 451, 454, 458, 461, 462, 464, 467, 477, 478, 482, 485, 486, 493, 498, 499, 501, 502, 506, 508, 509, 511, 512, 518, 520, 521, 523, 527, 532, 535, 538, 543, 544, 547, 549, 554, 557, 567, 571, 574, 576, 582, 584, 587, 589, 598, 603, 604, 609, 613, 620, 625, 630, 632, 643, 647, 651, 655, 657, 661, 664, 667, 668, 669, 672, 675, 680, 681, 682, 683, 684, 685, 686, 688, 691, 699, 711, 718, 726, 728, 735, 736, 742, 745, 746, 751, 752, 753, 756, 757, 759, 761, 764, 767, 773, 776, 779, 794, 797, 801, 807, 808, 815, 821, 830, 832, 833, 839, 843, 845, 849, 853, 863, 865, 872, 873, 875, 879, 881, 884, 886, 895, 896, 902, 908, 911, 913, 915, 919, 923, 926, 930, 935, 937, 940, 941, 943, 946, 949, 951, 957, 963, 970, 972, 975, 978, 983]\n",
      "\n",
      "=== Details of how a single model predicted correctly and other models predicted incorrectly ===\n",
      "Example of model A getting it right and other models getting it wrong Quantity: 46 (4.66%), subscript: [26, 58, 61, 121, 122, 145, 174, 218, 234, 258, 267, 281, 301, 311, 341, 352, 368, 385, 387, 392, 400, 453, 456, 474, 497, 540, 555, 556, 612, 666, 676, 710, 744, 766, 785, 817, 846, 847, 848, 859, 860, 916, 924, 959, 960, 962]\n",
      "Example of model B getting it right and other models getting it wrong Quantity: 45 (4.55%), subscript: [1, 32, 51, 88, 92, 102, 118, 120, 136, 261, 265, 323, 345, 359, 371, 379, 416, 418, 442, 452, 488, 492, 503, 505, 507, 524, 588, 595, 656, 663, 692, 696, 709, 775, 780, 798, 803, 831, 862, 871, 890, 905, 914, 944, 985]\n",
      "Example of model C getting it right and other models getting it wrong Quantity: 63 (6.38%), subscript: [25, 29, 62, 84, 96, 106, 127, 141, 172, 184, 196, 238, 257, 264, 271, 299, 307, 313, 317, 326, 353, 356, 372, 377, 396, 404, 413, 415, 432, 469, 489, 504, 541, 572, 593, 605, 618, 638, 652, 654, 700, 714, 729, 758, 770, 781, 787, 799, 819, 820, 829, 834, 840, 844, 856, 877, 887, 888, 912, 927, 928, 956, 966]\n",
      "\n",
      "=== Individual models predicted wrong and other models predicted correct details ===\n",
      "Example of model A getting it wrong and other models getting it right Quantity: 85 (8.60%), subscript: [9, 21, 30, 33, 42, 46, 53, 55, 79, 90, 94, 105, 116, 119, 130, 138, 142, 144, 182, 186, 194, 202, 203, 216, 220, 223, 229, 232, 254, 262, 272, 290, 312, 328, 355, 360, 364, 373, 382, 399, 417, 419, 425, 436, 437, 448, 462, 467, 478, 498, 508, 532, 557, 574, 576, 613, 647, 661, 669, 672, 680, 683, 686, 728, 736, 745, 751, 752, 756, 759, 794, 797, 801, 815, 830, 843, 881, 886, 895, 896, 915, 943, 972, 978, 983]\n",
      "Example of model B getting it wrong and other models getting it right Quantity: 44 (4.45%), subscript: [7, 56, 82, 110, 128, 131, 160, 166, 217, 227, 235, 255, 270, 304, 319, 381, 454, 464, 477, 482, 485, 493, 509, 567, 589, 598, 609, 632, 643, 655, 664, 668, 684, 688, 761, 764, 853, 873, 884, 908, 923, 930, 935, 941]\n",
      "Example of model C getting it wrong and other models getting it right Quantity: 138 (13.97%), subscript: [5, 37, 40, 41, 47, 48, 57, 68, 74, 85, 99, 100, 109, 111, 117, 140, 163, 165, 171, 173, 175, 177, 181, 189, 192, 193, 195, 198, 204, 205, 225, 230, 231, 233, 240, 249, 283, 288, 292, 308, 316, 318, 327, 333, 335, 339, 340, 375, 378, 384, 393, 394, 407, 409, 410, 422, 427, 430, 444, 451, 458, 461, 486, 499, 501, 502, 506, 511, 512, 518, 520, 521, 523, 527, 535, 538, 543, 544, 547, 549, 554, 571, 582, 584, 587, 603, 604, 620, 625, 630, 651, 657, 667, 675, 681, 682, 685, 691, 699, 711, 718, 726, 735, 742, 746, 753, 757, 767, 773, 776, 779, 807, 808, 821, 832, 833, 839, 845, 849, 863, 865, 872, 875, 879, 902, 911, 913, 919, 926, 937, 940, 946, 949, 951, 957, 963, 970, 975]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/y_h_2twd02q4y_zk07g55xk40000gp/T/ipykernel_66546/835670096.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  results = [torch.load(f) for f in result_files]\n"
     ]
    }
   ],
   "source": [
    "result_files = [hy_c10_test_analysis_path, nt_c10_test_analysis_path, cd_c10_test_analysis_path]\n",
    "compare_predictions(result_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pass in two models for more detailed analysis\n",
    "'''\n",
    "def compare_predictions_two_models(result_files):\n",
    "    results = [torch.load(f) for f in result_files]\n",
    "    num_samples = len(results[0][\"prediction\"])\n",
    "\n",
    "    all_correct_indices = []\n",
    "    all_incorrect_indices = []\n",
    "    one_correct_rest_incorrect = []\n",
    "    one_incorrect_rest_correct = []\n",
    "    \n",
    "    a_correct_b_incorrect = []  # Model a is right, model b is wrong\n",
    "    b_correct_a_incorrect = []  # Model b is right, model a is wrong\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        label = results[0][\"label\"][i]\n",
    "        predictions = [result[\"prediction\"][i] for result in results]\n",
    "        correct_preds = [pred == label for pred in predictions]\n",
    "        num_correct = sum(correct_preds)\n",
    "\n",
    "        if num_correct == 2:\n",
    "            all_correct_indices.append(i)\n",
    "        \n",
    "        elif num_correct == 0:\n",
    "            all_incorrect_indices.append(i)\n",
    "        \n",
    "        elif num_correct == 1:\n",
    "            one_correct_rest_incorrect.append(i)\n",
    "            if correct_preds[0] and not correct_preds[1]:  \n",
    "                a_correct_b_incorrect.append(i)\n",
    "            elif correct_preds[1] and not correct_preds[0]:\n",
    "                b_correct_a_incorrect.append(i)\n",
    "\n",
    "    def format_output(label, indices):\n",
    "        count = len(indices)\n",
    "        proportion = count / num_samples * 100\n",
    "        print(f\"{label} Quantity: {count} ({proportion:.2f}%), subscript: {indices}\")\n",
    "\n",
    "    print(\"\\n=== Comparison of the predictions of the two models ===\")\n",
    "    format_output(\"All models predict the correct example\", all_correct_indices)\n",
    "    format_output(\"All models predict examples of errors\", all_incorrect_indices)\n",
    "    format_output(\"An example of one correct prediction and the rest wrong\", one_correct_rest_incorrect)\n",
    "    format_output(\"Example of model a predicting correctly and Model b predicting incorrectly\", a_correct_b_incorrect)\n",
    "    format_output(\"Example of model b predicting correctly and Model a predicting incorrectly\", b_correct_a_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comparison of the predictions of the two models ===\n",
      "All models predict the correct example Quantity: 559 (56.58%), subscript: [2, 3, 4, 6, 8, 9, 10, 11, 13, 14, 15, 16, 19, 20, 21, 22, 23, 24, 27, 28, 30, 31, 33, 35, 36, 39, 42, 44, 46, 49, 50, 52, 53, 54, 55, 59, 60, 63, 64, 65, 66, 67, 69, 71, 73, 76, 77, 79, 80, 81, 83, 86, 89, 90, 91, 93, 94, 95, 97, 101, 103, 104, 105, 107, 108, 112, 113, 114, 115, 116, 119, 123, 124, 125, 126, 129, 130, 133, 134, 135, 138, 139, 142, 143, 144, 146, 147, 148, 149, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 164, 167, 168, 169, 170, 176, 178, 179, 180, 182, 183, 185, 186, 187, 188, 190, 191, 194, 197, 200, 201, 202, 203, 206, 207, 208, 209, 210, 212, 213, 215, 216, 220, 221, 222, 223, 224, 226, 228, 229, 232, 236, 237, 239, 242, 243, 244, 245, 246, 247, 250, 251, 252, 253, 254, 256, 260, 262, 263, 266, 268, 269, 272, 273, 274, 275, 276, 277, 278, 280, 282, 284, 285, 286, 287, 290, 291, 293, 294, 295, 296, 297, 300, 303, 306, 309, 310, 312, 314, 315, 320, 321, 322, 324, 325, 328, 329, 330, 331, 332, 334, 336, 337, 338, 342, 343, 344, 346, 347, 348, 351, 355, 357, 358, 360, 361, 363, 364, 365, 366, 370, 373, 374, 376, 380, 382, 383, 386, 388, 389, 390, 395, 397, 398, 399, 401, 402, 403, 405, 406, 412, 414, 417, 419, 420, 421, 423, 424, 425, 426, 428, 431, 433, 434, 436, 437, 438, 439, 440, 441, 443, 445, 446, 448, 450, 455, 457, 460, 462, 463, 465, 466, 467, 468, 470, 471, 472, 475, 476, 478, 479, 480, 483, 484, 487, 490, 491, 494, 495, 496, 498, 508, 510, 513, 514, 515, 516, 517, 519, 522, 526, 528, 529, 530, 532, 533, 534, 536, 537, 539, 542, 545, 550, 552, 553, 557, 558, 560, 561, 562, 563, 565, 568, 569, 573, 574, 575, 576, 579, 580, 581, 583, 585, 591, 594, 596, 597, 599, 600, 601, 602, 608, 611, 613, 614, 615, 616, 617, 621, 622, 623, 624, 626, 627, 628, 629, 631, 635, 637, 639, 641, 642, 644, 645, 646, 647, 648, 650, 653, 659, 660, 661, 662, 665, 669, 670, 671, 672, 674, 677, 680, 683, 686, 687, 689, 690, 693, 694, 697, 698, 702, 703, 705, 706, 707, 708, 712, 713, 715, 716, 719, 720, 721, 722, 724, 725, 727, 728, 730, 731, 732, 733, 734, 736, 739, 741, 743, 745, 747, 748, 749, 750, 751, 752, 754, 755, 756, 759, 760, 762, 763, 765, 768, 769, 771, 772, 774, 777, 778, 782, 783, 784, 786, 788, 789, 790, 791, 792, 794, 795, 796, 797, 800, 801, 802, 804, 805, 806, 809, 810, 811, 812, 813, 814, 815, 816, 818, 822, 823, 824, 825, 826, 827, 828, 830, 835, 836, 837, 838, 841, 842, 843, 850, 851, 852, 854, 855, 857, 858, 861, 864, 866, 867, 868, 870, 874, 876, 878, 880, 881, 882, 883, 885, 886, 889, 891, 892, 893, 894, 895, 896, 897, 899, 900, 901, 904, 906, 909, 910, 915, 917, 918, 920, 921, 922, 929, 931, 932, 933, 934, 938, 939, 942, 943, 947, 948, 952, 953, 954, 955, 958, 961, 964, 965, 967, 968, 969, 972, 973, 974, 976, 977, 978, 980, 981, 983, 984, 986, 987]\n",
      "All models predict examples of errors Quantity: 139 (14.07%), subscript: [0, 12, 17, 18, 26, 34, 38, 43, 45, 58, 61, 70, 72, 75, 78, 87, 98, 121, 122, 132, 137, 145, 150, 174, 199, 211, 214, 218, 219, 234, 241, 248, 258, 259, 267, 279, 281, 289, 298, 301, 302, 305, 311, 341, 349, 350, 352, 354, 362, 367, 368, 369, 385, 387, 391, 392, 400, 408, 411, 429, 435, 447, 449, 453, 456, 459, 473, 474, 481, 497, 500, 525, 531, 540, 546, 548, 551, 555, 556, 559, 564, 566, 570, 577, 578, 586, 590, 592, 606, 607, 610, 612, 619, 633, 634, 636, 640, 649, 658, 666, 673, 676, 678, 679, 695, 701, 704, 710, 717, 723, 737, 738, 740, 744, 766, 785, 793, 817, 846, 847, 848, 859, 860, 869, 898, 903, 907, 916, 924, 925, 936, 945, 950, 959, 960, 962, 971, 979, 982]\n",
      "An example of one correct prediction and the rest wrong Quantity: 290 (29.35%), subscript: [1, 5, 7, 25, 29, 32, 37, 40, 41, 47, 48, 51, 56, 57, 62, 68, 74, 82, 84, 85, 88, 92, 96, 99, 100, 102, 106, 109, 110, 111, 117, 118, 120, 127, 128, 131, 136, 140, 141, 160, 163, 165, 166, 171, 172, 173, 175, 177, 181, 184, 189, 192, 193, 195, 196, 198, 204, 205, 217, 225, 227, 230, 231, 233, 235, 238, 240, 249, 255, 257, 261, 264, 265, 270, 271, 283, 288, 292, 299, 304, 307, 308, 313, 316, 317, 318, 319, 323, 326, 327, 333, 335, 339, 340, 345, 353, 356, 359, 371, 372, 375, 377, 378, 379, 381, 384, 393, 394, 396, 404, 407, 409, 410, 413, 415, 416, 418, 422, 427, 430, 432, 442, 444, 451, 452, 454, 458, 461, 464, 469, 477, 482, 485, 486, 488, 489, 492, 493, 499, 501, 502, 503, 504, 505, 506, 507, 509, 511, 512, 518, 520, 521, 523, 524, 527, 535, 538, 541, 543, 544, 547, 549, 554, 567, 571, 572, 582, 584, 587, 588, 589, 593, 595, 598, 603, 604, 605, 609, 618, 620, 625, 630, 632, 638, 643, 651, 652, 654, 655, 656, 657, 663, 664, 667, 668, 675, 681, 682, 684, 685, 688, 691, 692, 696, 699, 700, 709, 711, 714, 718, 726, 729, 735, 742, 746, 753, 757, 758, 761, 764, 767, 770, 773, 775, 776, 779, 780, 781, 787, 798, 799, 803, 807, 808, 819, 820, 821, 829, 831, 832, 833, 834, 839, 840, 844, 845, 849, 853, 856, 862, 863, 865, 871, 872, 873, 875, 877, 879, 884, 887, 888, 890, 902, 905, 908, 911, 912, 913, 914, 919, 923, 926, 927, 928, 930, 935, 937, 940, 941, 944, 946, 949, 951, 956, 957, 963, 966, 970, 975, 985]\n",
      "Example of model a predicting correctly and Model b predicting incorrectly Quantity: 183 (18.52%), subscript: [1, 5, 32, 37, 40, 41, 47, 48, 51, 57, 68, 74, 85, 88, 92, 99, 100, 102, 109, 111, 117, 118, 120, 136, 140, 163, 165, 171, 173, 175, 177, 181, 189, 192, 193, 195, 198, 204, 205, 225, 230, 231, 233, 240, 249, 261, 265, 283, 288, 292, 308, 316, 318, 323, 327, 333, 335, 339, 340, 345, 359, 371, 375, 378, 379, 384, 393, 394, 407, 409, 410, 416, 418, 422, 427, 430, 442, 444, 451, 452, 458, 461, 486, 488, 492, 499, 501, 502, 503, 505, 506, 507, 511, 512, 518, 520, 521, 523, 524, 527, 535, 538, 543, 544, 547, 549, 554, 571, 582, 584, 587, 588, 595, 603, 604, 620, 625, 630, 651, 656, 657, 663, 667, 675, 681, 682, 685, 691, 692, 696, 699, 709, 711, 718, 726, 735, 742, 746, 753, 757, 767, 773, 775, 776, 779, 780, 798, 803, 807, 808, 821, 831, 832, 833, 839, 845, 849, 862, 863, 865, 871, 872, 875, 879, 890, 902, 905, 911, 913, 914, 919, 926, 937, 940, 944, 946, 949, 951, 957, 963, 970, 975, 985]\n",
      "Example of model b predicting correctly and Model a predicting incorrectly Quantity: 107 (10.83%), subscript: [7, 25, 29, 56, 62, 82, 84, 96, 106, 110, 127, 128, 131, 141, 160, 166, 172, 184, 196, 217, 227, 235, 238, 255, 257, 264, 270, 271, 299, 304, 307, 313, 317, 319, 326, 353, 356, 372, 377, 381, 396, 404, 413, 415, 432, 454, 464, 469, 477, 482, 485, 489, 493, 504, 509, 541, 567, 572, 589, 593, 598, 605, 609, 618, 632, 638, 643, 652, 654, 655, 664, 668, 684, 688, 700, 714, 729, 758, 761, 764, 770, 781, 787, 799, 819, 820, 829, 834, 840, 844, 853, 856, 873, 877, 884, 887, 888, 908, 912, 923, 927, 928, 930, 935, 941, 956, 966]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/y_h_2twd02q4y_zk07g55xk40000gp/T/ipykernel_66546/2872384906.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  results = [torch.load(f) for f in result_files]\n"
     ]
    }
   ],
   "source": [
    "result_files_1 = [nt_c10_test_analysis_path, cd_c10_test_analysis_path]\n",
    "compare_predictions_two_models(result_files_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comparison of the predictions of the two models ===\n",
      "All models predict the correct example Quantity: 518 (52.43%), subscript: [2, 3, 4, 6, 7, 8, 10, 11, 13, 14, 15, 16, 19, 20, 22, 23, 24, 27, 28, 31, 35, 36, 39, 44, 49, 50, 52, 54, 56, 59, 60, 63, 64, 65, 66, 67, 69, 71, 73, 76, 77, 80, 81, 82, 83, 86, 89, 91, 93, 95, 97, 101, 103, 104, 107, 108, 110, 112, 113, 114, 115, 123, 124, 125, 126, 128, 129, 131, 133, 134, 135, 139, 143, 146, 147, 148, 149, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 166, 167, 168, 169, 170, 176, 178, 179, 180, 183, 185, 187, 188, 190, 191, 197, 200, 201, 206, 207, 208, 209, 210, 212, 213, 215, 217, 221, 222, 224, 226, 227, 228, 235, 236, 237, 239, 242, 243, 244, 245, 246, 247, 250, 251, 252, 253, 255, 256, 260, 263, 266, 268, 269, 270, 273, 274, 275, 276, 277, 278, 280, 282, 284, 285, 286, 287, 291, 293, 294, 295, 296, 297, 300, 303, 304, 306, 309, 310, 314, 315, 319, 320, 321, 322, 324, 325, 329, 330, 331, 332, 334, 336, 337, 338, 342, 343, 344, 346, 347, 348, 351, 357, 358, 361, 363, 365, 366, 370, 374, 376, 380, 381, 383, 386, 388, 389, 390, 395, 397, 398, 401, 402, 403, 405, 406, 412, 414, 420, 421, 423, 424, 426, 428, 431, 433, 434, 438, 439, 440, 441, 443, 445, 446, 450, 454, 455, 457, 460, 463, 464, 465, 466, 468, 470, 471, 472, 475, 476, 477, 479, 480, 482, 483, 484, 485, 487, 490, 491, 493, 494, 495, 496, 509, 510, 513, 514, 515, 516, 517, 519, 522, 526, 528, 529, 530, 533, 534, 536, 537, 539, 542, 545, 550, 552, 553, 558, 560, 561, 562, 563, 565, 567, 568, 569, 573, 575, 579, 580, 581, 583, 585, 589, 591, 594, 596, 597, 598, 599, 600, 601, 602, 608, 609, 611, 614, 615, 616, 617, 621, 622, 623, 624, 626, 627, 628, 629, 631, 632, 635, 637, 639, 641, 642, 643, 644, 645, 646, 648, 650, 653, 655, 659, 660, 662, 664, 665, 668, 670, 671, 674, 677, 684, 687, 688, 689, 690, 693, 694, 697, 698, 702, 703, 705, 706, 707, 708, 712, 713, 715, 716, 719, 720, 721, 722, 724, 725, 727, 730, 731, 732, 733, 734, 739, 741, 743, 747, 748, 749, 750, 754, 755, 760, 761, 762, 763, 764, 765, 768, 769, 771, 772, 774, 777, 778, 782, 783, 784, 786, 788, 789, 790, 791, 792, 795, 796, 800, 802, 804, 805, 806, 809, 810, 811, 812, 813, 814, 816, 818, 822, 823, 824, 825, 826, 827, 828, 835, 836, 837, 838, 841, 842, 850, 851, 852, 853, 854, 855, 857, 858, 861, 864, 866, 867, 868, 870, 873, 874, 876, 878, 880, 882, 883, 884, 885, 889, 891, 892, 893, 894, 897, 899, 900, 901, 904, 906, 908, 909, 910, 917, 918, 920, 921, 922, 923, 929, 930, 931, 932, 933, 934, 935, 938, 939, 941, 942, 947, 948, 952, 953, 954, 955, 958, 961, 964, 965, 967, 968, 969, 973, 974, 976, 977, 980, 981, 984, 986, 987]\n",
      "All models predict examples of errors Quantity: 138 (13.97%), subscript: [0, 1, 12, 17, 18, 32, 34, 38, 43, 45, 51, 70, 72, 75, 78, 87, 88, 92, 98, 102, 118, 120, 132, 136, 137, 150, 199, 211, 214, 219, 241, 248, 259, 261, 265, 279, 289, 298, 302, 305, 323, 345, 349, 350, 354, 359, 362, 367, 369, 371, 379, 391, 408, 411, 416, 418, 429, 435, 442, 447, 449, 452, 459, 473, 481, 488, 492, 500, 503, 505, 507, 524, 525, 531, 546, 548, 551, 559, 564, 566, 570, 577, 578, 586, 588, 590, 592, 595, 606, 607, 610, 619, 633, 634, 636, 640, 649, 656, 658, 663, 673, 678, 679, 692, 695, 696, 701, 704, 709, 717, 723, 737, 738, 740, 775, 780, 793, 798, 803, 831, 862, 869, 871, 890, 898, 903, 905, 907, 914, 925, 936, 944, 945, 950, 971, 979, 982, 985]\n",
      "An example of one correct prediction and the rest wrong Quantity: 332 (33.60%), subscript: [5, 9, 21, 25, 26, 29, 30, 33, 37, 40, 41, 42, 46, 47, 48, 53, 55, 57, 58, 61, 62, 68, 74, 79, 84, 85, 90, 94, 96, 99, 100, 105, 106, 109, 111, 116, 117, 119, 121, 122, 127, 130, 138, 140, 141, 142, 144, 145, 163, 165, 171, 172, 173, 174, 175, 177, 181, 182, 184, 186, 189, 192, 193, 194, 195, 196, 198, 202, 203, 204, 205, 216, 218, 220, 223, 225, 229, 230, 231, 232, 233, 234, 238, 240, 249, 254, 257, 258, 262, 264, 267, 271, 272, 281, 283, 288, 290, 292, 299, 301, 307, 308, 311, 312, 313, 316, 317, 318, 326, 327, 328, 333, 335, 339, 340, 341, 352, 353, 355, 356, 360, 364, 368, 372, 373, 375, 377, 378, 382, 384, 385, 387, 392, 393, 394, 396, 399, 400, 404, 407, 409, 410, 413, 415, 417, 419, 422, 425, 427, 430, 432, 436, 437, 444, 448, 451, 453, 456, 458, 461, 462, 467, 469, 474, 478, 486, 489, 497, 498, 499, 501, 502, 504, 506, 508, 511, 512, 518, 520, 521, 523, 527, 532, 535, 538, 540, 541, 543, 544, 547, 549, 554, 555, 556, 557, 571, 572, 574, 576, 582, 584, 587, 593, 603, 604, 605, 612, 613, 618, 620, 625, 630, 638, 647, 651, 652, 654, 657, 661, 666, 667, 669, 672, 675, 676, 680, 681, 682, 683, 685, 686, 691, 699, 700, 710, 711, 714, 718, 726, 728, 729, 735, 736, 742, 744, 745, 746, 751, 752, 753, 756, 757, 758, 759, 766, 767, 770, 773, 776, 779, 781, 785, 787, 794, 797, 799, 801, 807, 808, 815, 817, 819, 820, 821, 829, 830, 832, 833, 834, 839, 840, 843, 844, 845, 846, 847, 848, 849, 856, 859, 860, 863, 865, 872, 875, 877, 879, 881, 886, 887, 888, 895, 896, 902, 911, 912, 913, 915, 916, 919, 924, 926, 927, 928, 937, 940, 943, 946, 949, 951, 956, 957, 959, 960, 962, 963, 966, 970, 972, 975, 978, 983]\n",
      "Example of model a predicting correctly and Model b predicting incorrectly Quantity: 184 (18.62%), subscript: [5, 26, 37, 40, 41, 47, 48, 57, 58, 61, 68, 74, 85, 99, 100, 109, 111, 117, 121, 122, 140, 145, 163, 165, 171, 173, 174, 175, 177, 181, 189, 192, 193, 195, 198, 204, 205, 218, 225, 230, 231, 233, 234, 240, 249, 258, 267, 281, 283, 288, 292, 301, 308, 311, 316, 318, 327, 333, 335, 339, 340, 341, 352, 368, 375, 378, 384, 385, 387, 392, 393, 394, 400, 407, 409, 410, 422, 427, 430, 444, 451, 453, 456, 458, 461, 474, 486, 497, 499, 501, 502, 506, 511, 512, 518, 520, 521, 523, 527, 535, 538, 540, 543, 544, 547, 549, 554, 555, 556, 571, 582, 584, 587, 603, 604, 612, 620, 625, 630, 651, 657, 666, 667, 675, 676, 681, 682, 685, 691, 699, 710, 711, 718, 726, 735, 742, 744, 746, 753, 757, 766, 767, 773, 776, 779, 785, 807, 808, 817, 821, 832, 833, 839, 845, 846, 847, 848, 849, 859, 860, 863, 865, 872, 875, 879, 902, 911, 913, 916, 919, 924, 926, 937, 940, 946, 949, 951, 957, 959, 960, 962, 963, 970, 975]\n",
      "Example of model b predicting correctly and Model a predicting incorrectly Quantity: 148 (14.98%), subscript: [9, 21, 25, 29, 30, 33, 42, 46, 53, 55, 62, 79, 84, 90, 94, 96, 105, 106, 116, 119, 127, 130, 138, 141, 142, 144, 172, 182, 184, 186, 194, 196, 202, 203, 216, 220, 223, 229, 232, 238, 254, 257, 262, 264, 271, 272, 290, 299, 307, 312, 313, 317, 326, 328, 353, 355, 356, 360, 364, 372, 373, 377, 382, 396, 399, 404, 413, 415, 417, 419, 425, 432, 436, 437, 448, 462, 467, 469, 478, 489, 498, 504, 508, 532, 541, 557, 572, 574, 576, 593, 605, 613, 618, 638, 647, 652, 654, 661, 669, 672, 680, 683, 686, 700, 714, 728, 729, 736, 745, 751, 752, 756, 758, 759, 770, 781, 787, 794, 797, 799, 801, 815, 819, 820, 829, 830, 834, 840, 843, 844, 856, 877, 881, 886, 887, 888, 895, 896, 912, 915, 927, 928, 943, 956, 966, 972, 978, 983]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/y_h_2twd02q4y_zk07g55xk40000gp/T/ipykernel_66546/2872384906.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  results = [torch.load(f) for f in result_files]\n"
     ]
    }
   ],
   "source": [
    "result_files_2 = [hy_c10_test_analysis_path, cd_c10_test_analysis_path]\n",
    "compare_predictions_two_models(result_files_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comparison of the predictions of the two models ===\n",
      "All models predict the correct example Quantity: 559 (56.58%), subscript: [2, 3, 4, 6, 8, 9, 10, 11, 13, 14, 15, 16, 19, 20, 21, 22, 23, 24, 27, 28, 30, 31, 33, 35, 36, 39, 42, 44, 46, 49, 50, 52, 53, 54, 55, 59, 60, 63, 64, 65, 66, 67, 69, 71, 73, 76, 77, 79, 80, 81, 83, 86, 89, 90, 91, 93, 94, 95, 97, 101, 103, 104, 105, 107, 108, 112, 113, 114, 115, 116, 119, 123, 124, 125, 126, 129, 130, 133, 134, 135, 138, 139, 142, 143, 144, 146, 147, 148, 149, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 164, 167, 168, 169, 170, 176, 178, 179, 180, 182, 183, 185, 186, 187, 188, 190, 191, 194, 197, 200, 201, 202, 203, 206, 207, 208, 209, 210, 212, 213, 215, 216, 220, 221, 222, 223, 224, 226, 228, 229, 232, 236, 237, 239, 242, 243, 244, 245, 246, 247, 250, 251, 252, 253, 254, 256, 260, 262, 263, 266, 268, 269, 272, 273, 274, 275, 276, 277, 278, 280, 282, 284, 285, 286, 287, 290, 291, 293, 294, 295, 296, 297, 300, 303, 306, 309, 310, 312, 314, 315, 320, 321, 322, 324, 325, 328, 329, 330, 331, 332, 334, 336, 337, 338, 342, 343, 344, 346, 347, 348, 351, 355, 357, 358, 360, 361, 363, 364, 365, 366, 370, 373, 374, 376, 380, 382, 383, 386, 388, 389, 390, 395, 397, 398, 399, 401, 402, 403, 405, 406, 412, 414, 417, 419, 420, 421, 423, 424, 425, 426, 428, 431, 433, 434, 436, 437, 438, 439, 440, 441, 443, 445, 446, 448, 450, 455, 457, 460, 462, 463, 465, 466, 467, 468, 470, 471, 472, 475, 476, 478, 479, 480, 483, 484, 487, 490, 491, 494, 495, 496, 498, 508, 510, 513, 514, 515, 516, 517, 519, 522, 526, 528, 529, 530, 532, 533, 534, 536, 537, 539, 542, 545, 550, 552, 553, 557, 558, 560, 561, 562, 563, 565, 568, 569, 573, 574, 575, 576, 579, 580, 581, 583, 585, 591, 594, 596, 597, 599, 600, 601, 602, 608, 611, 613, 614, 615, 616, 617, 621, 622, 623, 624, 626, 627, 628, 629, 631, 635, 637, 639, 641, 642, 644, 645, 646, 647, 648, 650, 653, 659, 660, 661, 662, 665, 669, 670, 671, 672, 674, 677, 680, 683, 686, 687, 689, 690, 693, 694, 697, 698, 702, 703, 705, 706, 707, 708, 712, 713, 715, 716, 719, 720, 721, 722, 724, 725, 727, 728, 730, 731, 732, 733, 734, 736, 739, 741, 743, 745, 747, 748, 749, 750, 751, 752, 754, 755, 756, 759, 760, 762, 763, 765, 768, 769, 771, 772, 774, 777, 778, 782, 783, 784, 786, 788, 789, 790, 791, 792, 794, 795, 796, 797, 800, 801, 802, 804, 805, 806, 809, 810, 811, 812, 813, 814, 815, 816, 818, 822, 823, 824, 825, 826, 827, 828, 830, 835, 836, 837, 838, 841, 842, 843, 850, 851, 852, 854, 855, 857, 858, 861, 864, 866, 867, 868, 870, 874, 876, 878, 880, 881, 882, 883, 885, 886, 889, 891, 892, 893, 894, 895, 896, 897, 899, 900, 901, 904, 906, 909, 910, 915, 917, 918, 920, 921, 922, 929, 931, 932, 933, 934, 938, 939, 942, 943, 947, 948, 952, 953, 954, 955, 958, 961, 964, 965, 967, 968, 969, 972, 973, 974, 976, 977, 978, 980, 981, 983, 984, 986, 987]\n",
      "All models predict examples of errors Quantity: 139 (14.07%), subscript: [0, 12, 17, 18, 26, 34, 38, 43, 45, 58, 61, 70, 72, 75, 78, 87, 98, 121, 122, 132, 137, 145, 150, 174, 199, 211, 214, 218, 219, 234, 241, 248, 258, 259, 267, 279, 281, 289, 298, 301, 302, 305, 311, 341, 349, 350, 352, 354, 362, 367, 368, 369, 385, 387, 391, 392, 400, 408, 411, 429, 435, 447, 449, 453, 456, 459, 473, 474, 481, 497, 500, 525, 531, 540, 546, 548, 551, 555, 556, 559, 564, 566, 570, 577, 578, 586, 590, 592, 606, 607, 610, 612, 619, 633, 634, 636, 640, 649, 658, 666, 673, 676, 678, 679, 695, 701, 704, 710, 717, 723, 737, 738, 740, 744, 766, 785, 793, 817, 846, 847, 848, 859, 860, 869, 898, 903, 907, 916, 924, 925, 936, 945, 950, 959, 960, 962, 971, 979, 982]\n",
      "An example of one correct prediction and the rest wrong Quantity: 290 (29.35%), subscript: [1, 5, 7, 25, 29, 32, 37, 40, 41, 47, 48, 51, 56, 57, 62, 68, 74, 82, 84, 85, 88, 92, 96, 99, 100, 102, 106, 109, 110, 111, 117, 118, 120, 127, 128, 131, 136, 140, 141, 160, 163, 165, 166, 171, 172, 173, 175, 177, 181, 184, 189, 192, 193, 195, 196, 198, 204, 205, 217, 225, 227, 230, 231, 233, 235, 238, 240, 249, 255, 257, 261, 264, 265, 270, 271, 283, 288, 292, 299, 304, 307, 308, 313, 316, 317, 318, 319, 323, 326, 327, 333, 335, 339, 340, 345, 353, 356, 359, 371, 372, 375, 377, 378, 379, 381, 384, 393, 394, 396, 404, 407, 409, 410, 413, 415, 416, 418, 422, 427, 430, 432, 442, 444, 451, 452, 454, 458, 461, 464, 469, 477, 482, 485, 486, 488, 489, 492, 493, 499, 501, 502, 503, 504, 505, 506, 507, 509, 511, 512, 518, 520, 521, 523, 524, 527, 535, 538, 541, 543, 544, 547, 549, 554, 567, 571, 572, 582, 584, 587, 588, 589, 593, 595, 598, 603, 604, 605, 609, 618, 620, 625, 630, 632, 638, 643, 651, 652, 654, 655, 656, 657, 663, 664, 667, 668, 675, 681, 682, 684, 685, 688, 691, 692, 696, 699, 700, 709, 711, 714, 718, 726, 729, 735, 742, 746, 753, 757, 758, 761, 764, 767, 770, 773, 775, 776, 779, 780, 781, 787, 798, 799, 803, 807, 808, 819, 820, 821, 829, 831, 832, 833, 834, 839, 840, 844, 845, 849, 853, 856, 862, 863, 865, 871, 872, 873, 875, 877, 879, 884, 887, 888, 890, 902, 905, 908, 911, 912, 913, 914, 919, 923, 926, 927, 928, 930, 935, 937, 940, 941, 944, 946, 949, 951, 956, 957, 963, 966, 970, 975, 985]\n",
      "Example of model a predicting correctly and Model b predicting incorrectly Quantity: 183 (18.52%), subscript: [1, 5, 32, 37, 40, 41, 47, 48, 51, 57, 68, 74, 85, 88, 92, 99, 100, 102, 109, 111, 117, 118, 120, 136, 140, 163, 165, 171, 173, 175, 177, 181, 189, 192, 193, 195, 198, 204, 205, 225, 230, 231, 233, 240, 249, 261, 265, 283, 288, 292, 308, 316, 318, 323, 327, 333, 335, 339, 340, 345, 359, 371, 375, 378, 379, 384, 393, 394, 407, 409, 410, 416, 418, 422, 427, 430, 442, 444, 451, 452, 458, 461, 486, 488, 492, 499, 501, 502, 503, 505, 506, 507, 511, 512, 518, 520, 521, 523, 524, 527, 535, 538, 543, 544, 547, 549, 554, 571, 582, 584, 587, 588, 595, 603, 604, 620, 625, 630, 651, 656, 657, 663, 667, 675, 681, 682, 685, 691, 692, 696, 699, 709, 711, 718, 726, 735, 742, 746, 753, 757, 767, 773, 775, 776, 779, 780, 798, 803, 807, 808, 821, 831, 832, 833, 839, 845, 849, 862, 863, 865, 871, 872, 875, 879, 890, 902, 905, 911, 913, 914, 919, 926, 937, 940, 944, 946, 949, 951, 957, 963, 970, 975, 985]\n",
      "Example of model b predicting correctly and Model a predicting incorrectly Quantity: 107 (10.83%), subscript: [7, 25, 29, 56, 62, 82, 84, 96, 106, 110, 127, 128, 131, 141, 160, 166, 172, 184, 196, 217, 227, 235, 238, 255, 257, 264, 270, 271, 299, 304, 307, 313, 317, 319, 326, 353, 356, 372, 377, 381, 396, 404, 413, 415, 432, 454, 464, 469, 477, 482, 485, 489, 493, 504, 509, 541, 567, 572, 589, 593, 598, 605, 609, 618, 632, 638, 643, 652, 654, 655, 664, 668, 684, 688, 700, 714, 729, 758, 761, 764, 770, 781, 787, 799, 819, 820, 829, 834, 840, 844, 853, 856, 873, 877, 884, 887, 888, 908, 912, 923, 927, 928, 930, 935, 941, 956, 966]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/y_h_2twd02q4y_zk07g55xk40000gp/T/ipykernel_66546/2872384906.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  results = [torch.load(f) for f in result_files]\n"
     ]
    }
   ],
   "source": [
    "result_files_2 = [nt_c10_test_analysis_path, cd_c10_test_analysis_path]\n",
    "compare_predictions_two_models(result_files_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the models that are correct and the models that are wrong\n",
    "##### Take out the sequences in which all the 3 models are correct and the sequences in which all the 3 models are wrong, and carry out the sequence comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "def compare_predictions(result_files, test_file, output_dir=\"3model_result\"):\n",
    "    results = [torch.load(f) for f in result_files]\n",
    "    test_data = torch.load(test_file)\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    num_samples = len(results[0][\"prediction\"])\n",
    "    num_models = len(results) \n",
    "    \n",
    "    all_correct_indices = []\n",
    "    all_incorrect_indices = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        label = results[0][\"label\"][i]\n",
    "        predictions = [result[\"prediction\"][i] for result in results]\n",
    "        correct_preds = [pred == label for pred in predictions]\n",
    "        num_correct = sum(correct_preds)\n",
    "\n",
    "        if num_correct == num_models:\n",
    "            all_correct_indices.append(i)\n",
    "        \n",
    "        elif num_correct == 0:\n",
    "            all_incorrect_indices.append(i)\n",
    "    \n",
    "    all_correct_data = {key: [] for key in test_data.keys()}\n",
    "    for idx in all_correct_indices:\n",
    "        label_in_hyena = results[0][\"label\"][idx]\n",
    "        label_in_test = test_data[\"labels\"][idx]\n",
    "        if label_in_hyena == label_in_test:\n",
    "            for key in test_data.keys():\n",
    "                all_correct_data[key].append(test_data[key][idx])\n",
    "\n",
    "    # Save the samples that all models predict correctly\n",
    "    correct_save_path = os.path.join(output_dir, \"3model_all_correct.pt\")\n",
    "    torch.save(all_correct_data, correct_save_path)\n",
    "    print(f\"Saved all correct data to {correct_save_path}\")\n",
    "    \n",
    "    # Process samples where all models predict errors\n",
    "    all_wrong_data = {key: [] for key in test_data.keys()}\n",
    "    for idx in all_incorrect_indices:\n",
    "        label_in_hyena = results[0][\"label\"][idx]\n",
    "        label_in_test = test_data[\"labels\"][idx]\n",
    "        # pt and data/Hyena_602.pt are consistent\n",
    "        if label_in_hyena == label_in_test:\n",
    "            # Adds the data for each field of the corresponding subscript in test.pt to the corresponding list\n",
    "            for key in test_data.keys():\n",
    "                all_wrong_data[key].append(test_data[key][idx])\n",
    "\n",
    "    # Save samples where all models predict errors\n",
    "    wrong_save_path = os.path.join(output_dir, \"3model_all_wrong.pt\")\n",
    "    torch.save(all_wrong_data, wrong_save_path)\n",
    "    print(f\"Saved all wrong data to {wrong_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all correct data to 3model_result/3model_all_correct.pt\n",
      "Saved all wrong data to 3model_result/3model_all_wrong.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/y_h_2twd02q4y_zk07g55xk40000gp/T/ipykernel_20182/1604733828.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  results = [torch.load(f) for f in result_files]\n",
      "/var/folders/69/y_h_2twd02q4y_zk07g55xk40000gp/T/ipykernel_20182/1604733828.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load(test_file)\n"
     ]
    }
   ],
   "source": [
    "# 文件路径配置\n",
    "result_files = [hy_c10_test_analysis_path, nt_c10_test_analysis_path, cd_c10_test_analysis_path]\n",
    "test_file = nt_test_file_path\n",
    "compare_predictions(result_files, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len is: 474\n",
      "table head is :dict_keys(['gene_id', 'sequences', 'labels'])\n",
      "First row of data:  {'gene_id': 'ENSG00000254901', 'sequences': 'ATAAATAAATAAATAAGTGGGCCAGGTGCGGTGGCTCACGCCTATAATCCCAGAACTTTGGGATGCCAAGGTGGGCTGAGTGCTTGAGTACAGGAATTCACGACCAGCCTGGGCAACATGACAAGACCCCATATTTATAATTTTTTTTTTTAATTAGCTGGTCACAGGCTGGCCACAGTGGCTCACGCCTGTAATCCCAGGACTTTGGGAGGCCAAGGCAGGTGGATCACCTGTGATCAGGAGTTTGAGACCAGCTTGGCCAACATGGTGAAACTCTGTCTCTACTAAAAATACAAACATTAGCTGGGAATGGTGGCACGCACCTGTAATTCCAGCTACTCAGGAGGCTAAGGCAGAAGAATCGCTTGAACCTGGGAGGTGGAGGTTGCAGTGAGCCGAGATTGTGCCACTGCACTCCAGCCTGGGCAACAGAGTGAGACTCTGTCTCAAAAAAAAAAAAAAAAAAAAAATTAGCTGGGTGTGCTGGTGTGAGCGCGTATTCCTAGCTCCTCAGGAGGCTGAGGCAGGAGGATCACTTGAGCCCAGGAGGCAGAGGTTGCAGTGAGCTGAGATCACACCACTTTACTCTAGCCTGGGCAACAGAGCAAGATGCTGTCTCAAAAACAAAGAAAGAAAGAAAGAAAGAAAGAAACCTCTTCCAGAAGGCCAAACACCCAACATGTCTACACTCACTGCACCCAAGTTGGGGTGAGCAAATGTTTTAAATTCCCCTTCTCTTCTTAATTTGCATTTTCCAGATGTCCACCTGGTTGGGTCATAGTTTAACCAAATAAATCATTCGTTGGGATGGGAAAGCCAAGAGTGGGTTCAGCTTGCTCCGCTCACAGGAGCTGCCACAAAGTTCAGAGAGGGGCAGCCACCTGTTGGGGGGTCACACAGAGAGGAGGGAGAGGAAGCTGGGCCTCGTGACCCCAGCCTCTGTTCTCTTGGGGGACTGAGGCATGGCCTTGAGCTCCATCCTTTTTTGAGGGGCCTAGTGTTACTACCTGGCCTCAGCCTTCCCTCCTGCAAACCAGCTTCCTGCTCTCTAAGATGACATCCCAAGATCCTAACTGTCCTGAAATTCTGTTTGCTGGAGGCCAAGCTTCCCCCTTTACTGAACAACAGGAAGGGGACATGAGGGAACAAGCCTGCCTTTTTAGAAAGCCCTTTTCATCTCTGGAAATCAGTCCTCACACACTGGTTCTCCGAAGTCGAAATACTTTTTTGTATCCAGTTTCCTTCACTCCAAGAGTCTGAGAGTTCTTGTCCCGAATGCCCAAGGTCCAGGGGCCCTGCCCTTCCCCAGGGATGATTCTTGGCCAGGCTAGCCTGAGGCCTTGATTATTGTTTTGTTTCTTTCTTCTTTTTTTTTTTTTTTTTTTTTTGAGATGGAGTTTCACTCTTGTTGCCCAGGCTGGAGTGCAATGACACGATCTTCGCTCACCACAACCTCCGCCTCCTGGGTTCAAGCGATTCTCCTGCCTCAGCCTCCTGAGTCGCTGGGATTACAAGCATGTACCATCATGGCCGGCTAATTTTGTATTTTTAGTAGAGATGGGGTTTCTCCATGTTGGTCAGGCTGGTCTCGAACTCCTGACCTCAGGTGATCCGCCCGCCTCAGCCTCCCAAAGTGCTTGGATTACAGGTGTGAGCCACCGCGCCTGGCCATTTTTTTGTTTCTTTTCTTTCTTTTTGTTTTTCTTTTAGAGACAGGATCTTGCTCTGTCACCCAGGCTGCAGTGCAGTAGCTCAGTCATAGCTCACTGCAGACTCGAACTCCTCGGATCAAGTGATCCTCCTGCCTCAGCCTCCCGAGTAGCTGGGACTATACACATAAGCCACCACACCTGCCTAATTAAAAAAACAATTTTTTTTGGCCAGATGCGGTGGCTTACTCTTGTAATCCTAGCACTTTGGGAGGCTGAGGCAGGCAGATCACATGAGACCAGCCTGGCAAACATGGTGAAACAACGTCTCTACAAAAAATGCAAAAATTAGCTGGTCATGGTGGCACACGCTTGTAGTCCCAGATACTCGGGAGGCTGAGGCAGAAAATCAATCACTTGAACCTGGGAGGCAGAGGTTGCAGTGAGCCAAGATTGAGCCACTGCGCTCCAGCCTGGGTGACAGAGTGAGACTCTGTCTCAAAAAAAATTTTTTTTTAAAGCCAGGTGCAGGGGCTCACGCCTGTAATCCCAGCACTTTGGGAGGTTGAGGCAGGTGAATCATTTGAGGTCTGGAGTTCAAGACCAGCCTGGCCAACATGGTGAAAGCCCATCTCTACTAAAAATACAAAAACTAGCCAGGCGGTAGTGGCACGCGCCTGTAATCCCAGCTACTTAGAAGGCTGAGGGAGGAGAATTGTTTGAGCCTGGGAGCTGGAGGTTGCAGTGAACAAAGATCACCACTGTACTCCAGCCTGGGCGACAGGATGAGACCCTGTCTCAAAAAAAAAACAAAGAAAGAAAAGAAAAGAAAAAGAAAATGGGGGTCTCACTATGTTGCCCAGGCTGGTCTCAAACTACTGGGCTCAAGTGATCCTCCTGCCTCAGCCTCCAAAGTGCTGGGATTACAGGCATGAGCCACAGTGCCCGGCCAGCTATTTCTGACTAATTGGGTCTGCAACTAGGGGTTAGGGTGGCCAAGGACACAGGTCACCCATATCTGCAGCCCTGGACCCCAAAAGAGATAGGGAGGGAGGACTGTGACCGTGAAAGTAATGCTGCCACCACCTTGTCATGTGCTAACAACCACACGGGGATGTGACTCTTGTTCCTGTACCCATTGAGGCAAAGGGAGGAGAGTTGGCCCCGGCAGCTGTTGCTCCCCCGTTCCAAGGTTTTGTGCCTACCATCCTGCTGTGACAAACGAGTGTACACCCACAGCAAGAAGGCATAGGGTTGTGCACAGGGGATGCGCTAATGCTAAAGGAGTGCACACCAGGAGAGAGGGACATCACATGCCACATCACAGAGCTCACAGACACATAATAGGGCCTCACAGCACCATGATTTTGGACACACAGCTAACTCTCCAACTCCCCAACTCACACCAAATCTCAGACGCAGAGCCCTTGGACACACAACTTCCAGACCCACACCCTTCAGACACACACCCCTTGGAGACACAACCCCCTCCTGCCACAACCTCGGGGACTCACCACCCCACGACACACACCCCTTTAGACACACACCCCACCCTGGGACGCCCAGCAGTACACCCGGACCTCCAAAACCCACCCAGCCCGCAGAAGCCGTTCAGAGGACACCACAGGAGTCTCAACCCGATATGACACGCACCTGCTCGGCCTGGGCCCTGGGACGCTGGGCGCACGGACCCGCGGCGGCTGCACGAAGATCAGGGGCCCGCGGCCGCAGGTGCCTTGGGAGCGGGCGGTCCTGCCCCCGGGAGCTGATTGGGCGGGACGGGCTTATTTGCATACAGGGCGTCACCCTGGAGGCGGCGATGGGCGTGCACTTGGGCGTGGCCCCGACCCGGGAGTTCATGGGCCTGGGACTCAGACGCGAAAGGATGCTGGGGCGCCATGGGTGGGGGCGCGGATAAGGAAGAAGCAGAACGGAGGGAGGGGGCTGGATCAGACTGGCCAGGCTTTCCCCGCCCTCACTGGGCGCCCTTGACCTTACTGCCGACCCCTGACCCTACTGTGACCCTAACCCTAATTCTAAACCTTTTCCTAACTCGGCCCAACTTGCCTGGATTGATTCCATCCTGAGTGAAATAGACTCCGATCCCGGACCCCGTCGCCCCGAGGAACCAAGCCCTTCTCGCAGGCAACGCTGACCCTCGCGCTCGCTAGAATGGAAAGACTGCAAGACGGGGCTCTCGGGTGGCCTGTGTGGCCTCACTCAACTGTGCCTCCTGGGGCCAGATATGTGCTCCCACAGGGAGTGACAGACCCTCCGCAGCTACTGGTCCCTCAAGCCCGCACCGCCTGCCCCTCCACAGTCCAAGACAACCACACTGCTTCCACCAACCACTTCCCTCTCCTTACCTCTCCCCGCTAAAGGCTGTTTTCTAAACTTCAAAGGTCAGCTGTGAGGGGCCTTCCAGGATTGGACCCCAGCCCAGAACTCTAAATCCACCCCTTCCCCTGGGATCCCAGCACAGGCCAGACCCACAGGAAGATCAGAGCAGGGGTTGGTGAATGTATCAATGTGAAAAGAATTACAGGATAAAGATGGCTCTGGGATGGGCACTTCTGGAGAAGCCCCCGGCCCGTCTGGGGCAGGCAGGGGATCCATGCACTCCCCTAACCCTGTGGGGTCAGGCTTGGGCCAGAGGAGGGATGGGAGGCTGGGACAGGCCAGAATTGGGGCAAGGGCTGTGCCAGAGGAGTCAGGGAGGGTTTCTCAGAAGAGAGGGCATTTGTGCTGGGCTTTCAAGTATGTGTAGGAGTTTGCCACTAGGAGAAAGGTTTTTCTTGTTTTTTTCTTCAAGGCAGGATCTCACTGTGTTGCCCAGGCTGGAGTGCAGTGGCATGATCATAGCTCACTGCAGCCTCGACCTCCTAGGCTCAAGCAATCCTCCCAACTTAGCCTCCCCAGCAGCTGGGACTACAGGCAGGCCACCACCAGGTCCAGCTAATTTTGTTTTTTTTAAGAGAGGATGTCTCACTATGTTGCCCAGGCTGGTCCCAAACTCCTGGGCTTAAGAGATCCTTCCTCCTCGGCCTCCCAAGGTGCTGAGATTACAGGTGTGAGCCACTACACAAGACCCGAGAAGGGCTTTTTCAAGAGGGAAAAGCACTTGCAAAAGTGCAGAGGAAGAGCATAGAACATTCTCTGGCCGGGGAAGATGAGGAAGGTGAAGCCAGTATAGCGGGCAGGGGGTGAAGAGGAGGGGCAGATACAGGTGGAGAGCAATGGGGCTGCAAAGCTGCCTGGTGTCCATCTCCTCCAGAGAAGAGAGGGGCTGGTCCCCCAAACTCAGGGGGAGGCATGTGGCAGATGCTGGAGTGTGGGGGACACTGAGACAGGGAACAGGGGGCCTGGGAGGCTTGTGTCTCTCCCCACACCTCAGTTTACCCGTCTGTGACATGGGCAGTGTGGGCCTCTAGTCTGAGCACAACTTTCTCTTTGGACATGAAAACTATAGAGTTCATACTGTGTCATCAGTGGGTCCCTCCTGTCCCCATCCAGTGCCCAGTGCCATACACTGGGGTGCCCCAAGGACATGCCAGGGCCTGGGCCAGGCTGGGGGCTTCTGCCTCCACTCCATGGTGCAGAACAAGGCTTGGGGAGTGTGGAGGTGGCTGGGGAGCCCTGGGCCCCCGGAGGGCTGCTCTGTTTACCGGAGGCCACCCGCCCACCTCTGATGCAACAGGCAGGCTCTGGTTACGTGAGGCCGGCAGCTGGGCCAGCTCTGGGGGAAGGGCTAGCCCAGCAGAGCGAAGCTGGCTGGGGCTGGCCCTTATGCACCCCAACTCCCCAGCCCCCTCCTTCTGGTGGGCACCATCTTGTTATTCATGGTCTGTCCCGTGTCCTTTTGCCTCTGTTCCTATCCTATACCTTCCCCAACCACTAATTGTGTCCCCTAGCAGGGCCACCAAGAGTCTTAACCTCATTCTTCTCAGATGAAGGGACACCCTTAGTTGCTAGAGTAAGAGGTTGGAAGGTAGAAGGGAAGGTTGTGAGGAGGAAGGGATAGAAGGGTCCAGAGTGGGTGGTGAGGGAGAAGAGTGACCTGGGAAGCAGGTCTGGGAGATGGGGGAGGTGGTGGGTGGGGGATGGAGGAGAAGGCTAGGGTGGGAGGCTGGGGGAGGTGGAGAAGCTGCCCCCGCTGTCTCCCTCCTCCAGATTCTCTCCCCTCTCCCAGGAGAAGGCTTTGAAGGACCTTCCATGATAAGGATGTGTGACCCAGTAGGTAGGGAGGGCCCTGAGGTTAAAGAACCCTGAGTCCCCATCCCAGCTGTGCCGCCCACTCACTGTGTGACCTCAGGATGGTCACTGCACCCCTTGGGGCCTCTTTGCTCATATGAGAAGTGTGGATAGGCTGGGCGCGGTGGCTCACGCCTGTAATCCCAGCACTTTGGGAGGCCGAGGTGGGTGGATCCCGAGGTCAGGAGATCGAGACCATCCTCACTAACACGGTGAAACCCCGTCTCTACTGAAAATACAAAAAATTAGCCAGGCGTGGTGGCAGGCGCCTGTAGTCCCAGCTACTCGGGAGGCTGAGGCAGGAGAATGGCGTGAACCTGGGAGGCGGAGCTTGCAGTGAGCCAAGATCGTGCCACTGCACTCCAACCTGGGCGACAGAGCGAGACTCTGTCTCGAAAAAAAAAAGAAAAAAAAAAAAGCCAGGCATGGTGGCACACACCTGTAATCCCAGCTACTCAGGAGGCTGAGGCAGGAGAATTGCTTGAACCCGGGAGGTGGAGGTTGCAGTGAGCCAAGATCACGCCACTGTACTCCAGCCTGGGTGACAAGAGCAAGACTCTGTCTCAAAAAAGAAAAAAAAAAAAAAAAAAAGAAGAAGTGGGGATAATAGGAGCCCTTAAGGCCCACCTCCAATGTTCCCTCCTCCAAAAGCCTTCCTATCCTCTTACCACTTAGGATCCTTCCACCTCCCCACCCTTCCTTCCAGCTTAGCACTGATCCAGGGGGCGGGGTAGAGTATGTGGCTGGTTCTGTGTCCCCCAGCCTGGGGGTTCCTTGGGCCCGGGCCCAGTGCATGGTGGCACCAAAGGGGTGTCTCCACGGGTTTCTTTCTTTTCATTTTTTAGAGACAGAGTTGCTCTGTTGCCCAGGCTGGAGTGCAGTGGTACAATCATGGCTCACTGCAGCCTTGAACTCCTGGGCTCAAGTGATCCTCTCGCTTCCATCTACCAAGTAGCTGGGACTATAGGAATGCACTACCACACCTGGCTAATTTTTAAATTTTTTTTTGAGGTGCACTGGGTGTCTCGCTTGTTGCCCAGGCTGGTCTCGAACTCCTGGGCTTAAGCGATCCTCCTGCCTTGGCCTCCAAAAGAGCTAGGATTATGGATATGAGCCATTGCGCACAGCCTCCATGGGTTTCTTGAAGAACTGGAGAATCTCAAAGAAGGTCTAAGCAGTGAAAGGGAGTGGGGGATAAGCCTGGGGCACCCAGAAGCCTTGTTCAGGGATACTGTAGGGAAAAGAAAAAAAAAAAAAAAGGCTTTCCAGGGGCCTGAGTGTGCAGGACTCTCCGGGAACCCAGAGGAGTAGGTCTGAAATAAAAACCAGGACTCACACCAATGGCTAAGGACAAAAACCCAGCGGAAGTCTGATTCTTGTCCTGACCACTGTGGACGGAGCAGGGGGCAGGTCGCTGCCCTGTATGGGTCTCCTTTTACCCCATCTGCAGATCAGGGTTGGGATCCTTTCTGGGTGATCTCCAGCCTCGTGAGCACTATGCTTCTGTTTGGATGCCCCATGGCACGGAGAGCTCACTGTCTCTCTTCTAGCTGCCTATCTCTGGAGTTCTGCCGGGAGTTAAATTTTTTTTTTTTTTTTTTTTGAGACGGAGTCTGGCTCTGTCGCCAGGCTGGAGTGCAGTGGCGCAATCTCGGCTCACTGCAATCTCCACCTCCCGGGTTCAAGTCATTCTGCTGCCTCAGCCTCCCGAGTAGCTAGGATTACAGGTGCTCACCACAGCCAGCTAATTTTTGTATTTTTAGTAGAGATGGGGTTTCACCATGTTGGCCACGACGGTCTCAATCTCCTGACCTCGTAATCCACCCGCCTCGGGCTCCCAAAGTGCTGGGATTACAGGCGTGACCCACCATGCCCAGCTGGGTGTTTTTTGAAATGGAGTCTACCTCTGTCGCCCACGCTGGAGTGCAGTGGCGTGATCTCGGCTCACTGCAACCTCCACCTCCCGGGCTCAAGCGATTCTCCTGCCTCAGCCTCCAGAGTAGCTGGGGATTACAGGCATGCACCACACCCAGCTAATTTTTGTATTTTCAGTAGAGACGAGGTTTCATCATGTTGGCCAGGCTGGTCTCCAACTCCTGACGTCAAGTGATCCGCCCACCTCGGCCTCCGAAAGCGCTGGGATTACAGGTGTGAGCCCCCGCGCCTGGCCAGTAAATGCTATTGAGTGAGCTCTAGGAGTGCCCTTGCTCTCCACCCTGACAAGGGAGGCTGCGAGCTCTTGTTCTGAGAGCCCTCTGGCCTCTCAGAACCTCAGTTTGCTCTCTGAAAAATGGGAGCGAGGGCATATAGGGTGAAAAAGGGTGGAGGGTCACAGTGTGCCCACCAGGGCCAGCCAGGACTGCAGACAATGACCGCTAGGAGGCTGAACTTCCCGCCAACGCTTTGCCTCTTGCGCCCCCTGGCGGCTAGAGTGGGCGGGGCGCCCTCCAGGCAAAAAGGGCTATCGAGCATTCAAATGAGATGCAAATTAACCAGCGCCCCCACAAGCCCTTGGACTCGGGGTGACGAGGTGTCTGACAACCCAGAGTCTTGATCAGATCCCTTTTATCTATTTATTTATTTATTATTTTTTGAGACAGAGTCTCGCTCTGTCACCCAGGCTAGAGTGTAGTGGCTCCATCTTGGCTCACTGCAACCTCTGCCTCCCGGGTTCAAGCAATTCTCCTGCCTCAGCTTCCCAAGTAGCTGGGATTAGAGGCGCGCACCACCACACCCGGCTAATTTTTTATATTTTTGGTAGAGACAGGGTTTCACCATATTGGTCAGGCTGGTCTTGAACCCCTAACCTCAGGTGATCTGCCCACCTCAGCCTCCCAAAGTGCTGGGATTACAGGCGTGAGCCACCATGCTCAGCTATTTTGTTTTTTTATAGAGACAGGGTCTCTCCATGTTGCCAGGCTGGTCTCCAACTCCTGGACTCAGGGGTTCCTCTAGTCTTGACTTCCCAAAGTGCTGGGATTACAAGCGTGTGCCACCGCACCCGGCCTAGACTCCTTTTAAGAGATGACTGGGCTGGGCACGGTGGCTCACACCTGTAATCTCAGCACTTTGGGAGAACGAGGTCAGGAGTTCAAGACCAGTCTGGCCAAGATGGTGAAACCCCGTCTCTACTAAAACTACAAAAATTAGTTGGGCGCAGTGGCAGGTGCCTATAATCCCAGCTATTCTGGAGGCTGAGGTAGGAGAATGGCTTGAACCCAGGCGGCAGAGGTTATTGTGAGCCGAGATCGCGCCACTGCACTCCAGCCTGGGCGAGAGTGAGGCTCCAGCTCAAAAAAAAAAGAGAGAGAGATGACTGTTAGTCGCTGGTTGAGGTTGCATCCAGGGGCTATGAGTGACTCAGAGCCCTAGAAGGGTCTGGTGGGGCCCCCCACACCTGCCTCCCTCACCCCTAGCTCCTAGCGCCCCAGACATCCTAGGCAGGTCCTGGGAGGCAGGCAATAATCCCATTCTGCTGCCAGTTTGTCCTGAAGGTCTGAGCAAGAGGGCTGGCTCACAAATACCAGTGTTTGCTGGAGAAATGGTCGCACGAAGGACAGATGGACATTGAGAGGTGCCTTCAGAGCAGGTAAGAAGGGAGAAGGACTCTGGAGGCCACACACCAATCAGTGCCCTAAATGGGGTGGCTTTAGGCTCCTCCCCACTCCTAAGCTGAGTAGGACAGTGGAGAGGTGGGATAATGGCACAGAGGCCCCCCAGTGTGACCTAGAAGGAGGAGAGGTAGGGCTGAACCCCCTTTCCAGGCCTTCAGATGCCTTGACAGGCCCGATGTCCTAACAGGATGCCCTCTGTCTCCCCTCGGGACACCCCTGCTCTTCCCATCCGTCTCCATCAACCACCCCACAGTGCCAAGCTGAGGGACCCTCCCAGAGCAGTGACTGGAACCTGCCTCGACCCCAAACAACTCTTATCTGACCTCCCTAATACATTGTACTGGAACAAGGGATTCTGGGGTAACTGCATTGGCACACAGCTGGCCTTGGAGATGCATAACTTGTGGAGGCCTGGTCACCCAGGCTTGGCATTCACAGTGGTCAGTGCAGGCCCTTCTGCAAACAGCAGCAGAAACGGACTCAAAGCCATTTTGGCAAAAATCATAATTTAGGGGTCCATGAAACTGAAGAAATGAAATTGGCTAGTGTCAGGTACAGTTGGATCCAGGGCTCCACACAATGTTCTCAGGTCCCCCCGGAACCCCTCGCCCCGGCCTGGTTCCCATACACCCAGCTCAACCTTCCTGATGCTCCTGGTAGAACTCCCAGGAAAGGTCGCTCATTGGGTCACGTGTCCATCTCAAACCAATCACTAGGAACCTAGGACAAGGGAGCTCTGATTGGCCAGGCCGAGGTGCCACATCCACCACTGGAACTAGGGGTGGCTTCTCCATGTGGCCTGAGCCTGGGGTGGGCGTTGCTGCTTGAGGCAGAATGCAGGTTGGGAGGGTAGATCTGCGAATGTCCAACTCAGCCTGTAGAGAAGGTGCAGTCCAGCCTTCCTTATCCAGATGAAGAAAGGGAGAACCAAGGCTGGGTGCGGTGGCTCACACCTATAATCCCAGCACTTTGGGAGGCTGAGGCGGGAGGATCACGAGCTCAGGAGTTTGAGACCAGCCTAGCCAACATGGTGAAACCCCGTCTCTACAAAAAATACAAAAAATTAGCTGGGGGTAGTGGTGTGCGCCTGTAGTCCCAGCTACTTGGGAGGCTGAGACAGGAGGATCACTTGAGCCCAGGAGGCAGAGGTTGCAGTGAGCCGAGATCGTGCCACTGCACTCCAGCCTGGATGACAGCCAGATCCTGTCAAAAGAAAGAAAGAAAGAAGGAAGGAAGGAAGGAAGGAAGGAAGGAAGGAAGGAAGGAAGGAAGGAAGGAAGAGAAAAGAAAAGAAAAGAAAAGAAAAGAAAAGAAAAGAAAAGAAAAGAAAAGAAAAGAAAGGAAGAAAGAAAGGGAGACCCAGAGAGGGCCAGCACCAGGCCTATGTCACACAGCACATCAGAGGCAGGCAGGGCTGGGACCAAACCAAGCCGGGCTCTGCTCTTCTCTGACTGTGCCCTTTTTTTTTTTGGTTGGGGGGCGGCGGGTGCGAAGTCTCGCTCTGTCGCGCAGGCTGGAGTGCAGTGGCGCCATCTTGGCTCACTGGAACCTCTGCCTCCTGGGTTCAAGCGATTCTCATGCCTCAACCTCCTGAGTAGCTGGGATTACAGGTGCCCGCCATCACACCCGGCAGAGCCTCTTTTTGTCCATATGTATTTGGGGGTGAGACCTGCCTTTTGGGGCTGAGGATGAAATGAGATGACCCCTGTGAAGACTTTAGCCCAGGGCCAGGGACTTGATAGGTGCATAATCAGGATTATTTTGATGGCAGTTATTGGTCCAAGGGTCGATGGGGGGCCAGTGAGAGTCCAGGAGCAGCAAAGCCACTTCTGGACTTCTGGGAACATGGACGGCTGGGCCTGGAGGCTCCGGGCAGGTGCAGGAGGAATGCTGGCCACACTCGTTCAGTCTTCAGTCTCTGCCTGGCCCAGCTTCAGTCAGGCTGGGGAGCAGGGGCAAGGGCCAGGGTGGTCTGAGAGACAGACCCAAGGCATGAGGCAGCACCTGAGGGATGACCCCGGGCCAGACTAAGGTGGAGCTTGGTGGTCAGAGGTCCGGGTGGGAATCCTGGCATTTCTGGGCGTGGGCGATGACCCTGAGCTGGCTGCCTTACCTCTCGGGGCCTCAGTTTCCTCTTCTGTAAAACAGGGCAGTGCTTAAGGTCCTGGGACAGGAGAGGGGGATGACTTGACCCTGCGAAGGTCACTGATGACCCCAAAGAGGCGCAGTTTTGGGGGAAAAGTGGGAATGAAAGCCTCTGAGGCCGGGCGCAGTGGCTCATGCCTATAATCCCAGGACTTTGGGAGGCCGAGGCGGGCAGATCACCTGAGGTCGGGAGTTCGAGACCTGACTGACCAACATGCTGAAATTCCATCTCTACTAAAAATACAAAAATTAGCCGGGCGAGGTGGCAGGCACCTGTAGTCCCACCTACTCGGGAGGCTGAAGCGGGATAATCGCTTGAACCTGACAGGCAGAGGTTACACTGAACCGAGATCGCACCACCGCACTCTAGCCTGGGTGACAGAGTGAGACTCCATCTCAAAAAAAAAAAAAAGAGAGAAAAAAAAAGCCTCTGAGATGCGGCATGAGGCTGGCGGTTCCAGGGAGAGGCGAGGAACTCTGCAGGGACAAACGTGGATGAGACACAGAGCAATCCTGCCGGGCAGCAGCAGCGGAGGGGACAGCAGACGCTGGGAAGTGAGGAGGGCAGCGGGGCAGTGTCCTCGAGGCACAAAGTAGGAGGGACCAGGGTGTGGGGGATGGGACTGGGG', 'labels': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/y_h_2twd02q4y_zk07g55xk40000gp/T/ipykernel_20182/3715749460.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  correct_data = torch.load(correct_file_path)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "correct_file_path = '3model_result/3model_all_correct.pt'\n",
    "correct_data = torch.load(correct_file_path)\n",
    "\n",
    "print(f\"len is: {len(correct_data['labels'])}\")\n",
    "print(f\"table head is :{correct_data.keys()}\")\n",
    "first_row = {key: value[0] for key, value in correct_data.items()}\n",
    "print(\"First row of data: \", first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len is: 93\n",
      "table head is :dict_keys(['gene_id', 'sequences', 'labels'])\n",
      "First row of data:  {'gene_id': 'ENSG00000138380', 'sequences': 'TCCTCAACAAAATACTTGCAAACCAAATTCAACAACACATTAAGAAGATCATCATGACCAAGTAAGATTTATCCCAGGGATGCAAGAATAGTTCAACATACACAAATCAATGTAATACACTGTGATGGTTAACACTGAGTGTCAACTTGATTGGATTGAAGGATGCAAAGTATTGTTCCTGGGTGTGTCTGTGAAGGTGATGCCAAACGAGATGAACATTTGAGACAGTGGAATGGAAGAGGCAGACCTACCCTCTATCTGGGTAGGTACCATCTAATCAGCTGCCAGCATGGCTAGGATAAAAGGAGGCAGAGGAATGTGGAAGGACTAGACTGGATCAGTCTTCAGGCCTTTATCTTACTCCTGTGCTTCCTGCCCTCAAACATTGAACTCCAAGTTCTACAGCTTTTGGATTCTTGGACCAGTGGTTTGCCAGGGATTCTCAGGTGTTTGGCCATAGACTGAAGGCTGTACTGCCGGCTTCCCTACTTTTGAGGTTTTGGGACTCGGACTGGCCTCCTTGCTCCTCATCTTGCAGACAGCCTATCGTAGGACTTCATCTTGTGATCGTGTGAGTCAATACTCCTTAATAAACTCCCTCTCGGCTGTGCGCAGTGGCTCACACCTGTAATCCCAGTACTTTGGGAGGCCAAGAGTTCAAGACCAGCCTGACCAGCATGGTGAAACCCCGTCTCTACTAAAACTACAAAAATTAGCCAGCCGGGTGTGGTGGCACGCGCCTGTAATCCCAGCTACTCGGGAGGCTGAAGCAGGAGAATTGCTTGAACCCGGGAGGTGGAGGTTGCAGTGAGCCGAGATCGTGCCACTGCACTCCAGCCTGGGCGACAGAGCAAGACTCCGTCTCAAAAAAAACCAAAAAAACAAAAAATCTCTCTTTCATATATACATCTATTCTATTAGTCCTGTCCCTCTAAAGAACCCTAATATATACATCATATCAACAGAATGAAGGACAAAAAGCATATGATCATTTCAATTGATGCTGAAAAAGCATTTGATAAAATTCAACATCCTTTCATGATAAAAGCCCTCAAAAAACTGGGTATAGAAAAACATACTTCAACATAATAAAATTCATATATAATAGACCCCCATACTAAATGGGGAAAAAACTGAAAGCCTTTCCTCTAACATCTGGAACATGATAAGGATGCCCACTTTTACCACTGTTATTCAACACAGCAATGGAAGTCCTAGCTAGAGCAATCAGATGGAAGGAAGGAAGGAAGGAAGGAAGGAAGGAAGGAAGGAAGGAAGGAAGGAAGGAAGTGAGGGAGGGAGGGAGGGAGGGAGGAAGGGCATCCAAATTGGAGAGGAAGATGTCACATTATCCTTGTTTGCAGATGATTTGATCTTATATTTGGAAAAATCAATGATATAACGTTATATTTGATATAATCTTTATTTTACCAAAAACCAATTAGGACTGATAAATTCAGTAAAGTTGCATGATACAAAAATCAGTAGCATTTCTATATGCTAACAGCGAACAATCAGAAAAAGAAATCAAGAAAGTATTTCTACTTACAGTAGCCACAAATAAAATACCTAGGAATTAACCAAAGAAGTGAAAGATCTCTACAACAAAAACTATAAAACATCGATGAAAGACATTGAAGAGGGCTGGGTGCAGTGGCTCATGCCTGTAATCCCAGAACTTTGGGAGGCTGAGACGGGTGGATCACCTGAGGTCAGGAGCTCGTGACCAGCCTGGCCAACATGGTGAAACCAAGGCTGTACTAAAAATACAAAAATTAGCTGGGCGTGGTGGCAGGCACCTGTAATCCCAGCTACTCAGGAGGCTGAGGCAGGAGAATCGCTTGAAACCGGGAGGTGGAGGTCGCCATGAGGTGAGATTACGCAATTGCACTCCAACCTGGGCAACAAGAGTGAAACTCTGTCTCAAAAAAAAAAAAAAAAAAAAAAAAAAGAAAAGAAAAAGAAAAAAAGAAATTGAAGAGGACACAAAAAAATGGAAAGATATTCCAGGATCATGGATTGGAAGAATTTAATATTGTTAAAATGTCCATAGTACCCAAAGCAACCTACAGATTCAATGCAATCCTTATCAAAATACCAATGACATTCTTCACAGAAAAAAAAAAATCCTAAAATTTACATGGAACCACAAAAGAACCAGAATAGCCAAAGCTACCTTAAGCAAAAAGAACAAACTTGGAGGAATCACATTACCTGACTTCAAATTATACTACAGAGCTATAGTAACCAAAACAGCATGGTACTGGCATAAAAACAGACACATACACCAATGGAACAGAATAGAGAAGCCAAAACAAATCCACACATCTACAGTGAACTCAAACCTACACTGGGGAACAGCCAGTCTCTTCAATAATGGTGCTGGAAAAACTGGGTATCCATATGCAAAAAAAATGAAACTAGACCCCTATCTCTTGCCACATACAAAAGTCAAAACAAAAGGGATTAAAGACTTAAATATTTGTAAGACCTCGAACTATGAAACTAATACAAGAAAACATTGGGGAAACTCCCCAGGGCAGTGGTCTGGGCAAAAATTTCTTGAGTAATACCACACAAGCACAGGCAATCAAAGCAGAAATAGACAAATGCGATCCCATCAGGTTAAAAAGCACAGCAAAGGAAACAATTAACAAAGTGAAGAGACAACCCACAGAATGGGAATAATTATTTGCCAACTACCCATCTGACAAGGGATTAATAACCAGAATATATAAGGAGCTCAAAAAGCTCTGTAGGTAAAAATTTAATAATCTGATTAAAAATTGGGCAAAAGGTTTGAACATACATTTCTCAAAAGAAAATTCAAATGGCAAACAGGTAAATGAAAGGGTGCTCACCATCACTGATCATCAGTGAAATACAAATCAAAACTACAATGAGATATCATTTCACCCCAGATAAAATGGCGTTTTTCCCCACCCCTCCCAAGACGGAGTCCTGCTCTGTCACCCAGGTTGGAGTGCAGTGGCACGAGCTCGGCTCACTACAACTTCCGCCTCCCGGGTTCAAGCAATTCTCCTGCTTCAGCCTCCTGAGTAGCTGGGTTTACAGGCACCTGCCACCATGCCTGGCTAATTTTTGTATTTGTAGTAGAGATGGGGTTTACTATTTTGCCCAGGCTGGTCTCAAATTCTTGACTTCATGATCCACCCACCTTGGCCTCCCAAAGTGCTGGGATTACAGGCGTGGGCCACCACGCCAGGCCAAAATGGCTTTTATCTGAAAGTCACGCCATAAAAAATGCTGGTGAGGATATGGAGAAAAGGGACCCCTCATAAACCTTGGTGGGGATGTCAATTAGTACAACTACTATGGAGAACAGTTTGGAGATTCCTCAAAAAACTAAAAATAGAGCTACCATATGATCCAGCAATAGCACTGCTAGGTATATACTCAAAAGTAGAGAAATCAGGAAATCAATACTTCTACACAATGGAGTACTATTCAGCCATAAAAAAGAATGAGATCCTGTCATTTACAACAACATGGGTAGAATTGGAGGTCATTATGTTCAGTGAAATAAGCCAGGCACAGAAAACAAACTTTGCATATTCTCACTTATTTGTGGGAGCTAAAAATTAAAACATTGAACTCATGGAGATACAGAGTAGAAGGAGAGTTACCAGACACTGAGAAGGGTAGTGGGGGAAGTGAGAGGCAAGTGGAAATGGATGGTTAATGGGTACAAAAGAATTGAAAGAATAAATAAGATCTTGTATTTGATAGCACAACTGGATGACTATAGTCAATAATTTAATTATACATTTAAAAACAACGAAGAGTATAATTGGATTGTTTGTAACACAAAGGATAAATGCTTGAGGTGATGGATACCTCATTTACCCTGATGTGATTATTACACCTTATATGCCTGTATCAAAATATCTAATGTACCCCATAGATACACCTACCACGTACCTACAAAAATTAAAAAGAATATTTCTTATTCTAGTTCAAAGACTTGTTCAACAAATAGACTATATAACAGTTTTGAAATTTATAAAGGTAACTATGCATCAGAATCTCTATATATCCATGATGAAAACAAAAACAAGGAGCAACAGGAAATGCTATGTAAATATCCTTGAATATTGAATATTGAACTGTAACTAGCAATAAAAAGATTGCAAACCCTGTCCCCTCCTCCCCCACCCCAAGCCAATAAAGTTGCTCAAAAACATCACCTCAAATTTCACAAAAAAATTATGTGATTATTTGAAGACGAAGATGGGGAAAATACTACACATTAGTTTATAACTCTTACCTCCTCATTCCTCCAACTACGGTAGAGTCAGAAGAGCTCTGCAGCATTGGAAACATCTGCATAAAAATACACTTGCCTCTGGCTGGGCATGGTGGCTCACACATGTAATCCCAGCACTTTGGGAGGCCCAGGAGGGTGGATCACCTGAGGTCAGGAGTTCGAGACCAGCCTCGCCAAATGGTGAAACCCCGTTTCTACTAAAAGTACAAAAAATTAGCCAGGCATGGTAGCGGGCACCTGTAATTCCAGCTACTCTGGAGGATGAGGCAGGAGAATCACTTAATGTTTCATGTTTAAATTTTCTCTATTTTATGTAATGTTTCTTTACTTTTTAAAAAATACTTTATTATAGAAAATTTCATACATATGGAAAAGTTCAGAAATATCAATCAAAACTACAATGAGATACAGAAAATAATATAAGGAGCCTCCTTACACCTATCACCAAACTTCAACATTTTTTTTTTTTTTGAGACTTCCTCTATCACCCAGGCTGGAGTGCAATGGTGCGATCTTGGCTCACTGCAACCTCCGCCTCCCGGGTTCAAGCAATTCTCCTGCCTCAGCCTCCTTAGCAGCTGAGATTACAGGCATGTGCCACCACGCCTGGCTGATTTTTGTATTTTTAGTAGAGACGGGGTTTCACCATGTTGGTCAGGCTGGTCTCAAACTCCTGACCTCGTGATCCACCCGCCTCGGCCTCCCAAAGTGCTGGGATTACAAACTTCAACAATTATTAACTCATAGCCAATCTTATTTCATTGATATTTTTACATACATTGTTCATATTAATTTAATGGGCAAATAACTTATATTAGTACAATAGTATATATTTAATATTTGTATCTTTTAAATGTTTTATATTCTTCCATGTTTTGTTATATCCTTTTTTTATGAATTCTGGACACAAAAAATAAAATACTGGTATTCATACTGTCATTTTGATATTACACTAAACACTCAAATACTGATATTGTCATTTTGCTTTGTGCTTCACTAGTAAATGGGGCCTAAGCACTAAAACGCTTGTTTTATGCCTGCATAACTCAGCCCTCTGTAGAACAATCAATGTCTTGATTACCTCTATATTACAACTTTATACCAAAATCTTTATTCCTTAACTACAACCACCATATTTCCACATGGCCTTAAAAACAGATTAAACAGAAATTTTTTTCTGAAAACTTTTCTTTCAAAGCAGTATTTACAGTATCATCCAGCAGAAGAACCTGTACTTTATAACTTACAAATTTTACCACTACAAAATTCATACAGTATCATAAGGCCAAAAAAGGAACTAAAAACTTAATGCATTTTAGAATAATTAGGGACACTGTGCCCTCTCCTAAGCATATATACCCACCATTTTTGTCCTTTAGTAGTTTATTGATGATGTTACTAAGGTCGGCAATTTCAGAGGCAGCAGGGATTGAGAAGGGAACATCATCTACGGCATATCTATAAAAAGGAAATGATATGTCAAGATCAAGTCTACAGATATTTGAACAAACATGCTTTTACGAATTCAACATACATTTATTGGGTAAATAAATGCCAGACATCCAGACATTTTGCTACATGTTGTAAACAAAAATGAATTAAAACATGGTCTCTGTTCTAAGATGAGGTTGAAGTTTAGTAATCAAATAGAATGAAGTTATATAAGAACAATGCCATTAGGTGACATGTATAAAAAGAACAAAGACACCATAGGTCATTTTAACCACAGAAAAGGAGGGCAGGGGAGCAGGGCCAGAAACGATCCACAGAAAAGATAACTGTTAAGCAACATTTAGAAAAGTGATTAGCCAGGCGTGGTGGCATGTGCCCGTAGTCCCAGCTACTTGGGAGGCTGAGGTGGGAGGACTGCCTGAGCCCAGGAGGTCGAGGCTGCAGTGAGCTGAGATTATACCACTGCACTGCACTCCAGCCTGGGTGACACAGCAAGACCCTGTCTCAAAAGAGAAGTGAGTTTTTGCCAGGTGAGGGAAGGGTAGGCTGGAAGAAGCAGTGTGGACGAAACCAGAGAGAACATCAAAACAATAGCTAGGAGTTGTTAAACTGCCCAGCCACTTCCAGAAAATCTGACTTTGCTCCAAAGTGACCAAAGGCTAAAGAGATACACATGGAGTGCTTTATATATCATGTGAAATCATTTTTACTTCTTTTTTGTGGGTAATGAAGAACTATCACAGCAGTGCAAATTAGAAGTATCATCAGATTTTATTTAGTTAGATCTTTCTGGCTGCACAATGGGCACTGAAATAGAGGAACACATGACAGGAGGCAGATTAGGAGACTCTTATAAGAGAGACTGTGAGAAAATATTTGCAAATCATGTATCTAATAAGGAACTTGTATCTAGACTATATTAAGAACACTTACAACTCAATAATAAAAAGACAACACAAATTTTAAAATGGCCCAATAGGGACATGCAAATGCTCATCATTAGTCATCCAGGTAATGTTAATCAAACACAATGAAATATAATTTCACATCCACTAGGGTGCTATAAGTAAAAAGACAGCTAATAACAAGTGTCAGTGAAAATATGGAGCAATTGGAACCCTGATTATTGCTGGCAGTAACGTAAAATGGTGCAGTTGCTTTGGAAAACAGTCTGGCAGTTCCTCAAAAACTCAAACATGGAATTACATCTGACCCAAAAACTCCACTCCTAAGCATATATGCAAGAGAAACGAAAACATACATCCACACGAAAACTTGTACGAGTGTTCACATAACAACTATTCATAATAGCTAAAAGGTATAAACAACCTAAATGCCCACCAGCTGATGAATGGAAAAATATGGTACCTCCATTCAATGGCCTATTATTTGGCCACAAAAAGGAATGAAGTGCTAATATATACTATGATATAATGAAACCATTTTTTAAAAATCATAGTAAGTGAAAGAAGCCAGTCACAAAAGATCACATTATATAATTCCATTTATTCAAAACGTCCAGAAGAGAGAAAGTTACAGAGCTAGAAAGTAGATCAGTGGTTGCCTAGGGCTGGGGGTATTATTTAACAAACAGCTCAAGGATGATGAAAAATGTTCTAAAATTGACTGTGGTGATGGTTGCACAACTTTGCAAATATGATGCTGATGATCCTCAAACCACACTTTGAGCAATAAGGCTATAGCTATATATTTATGTATATAATTCACATATATGTATATTCTTGTGTGTGTGTGTGACAATTATATTAGCCCCCCGCCTCCATTTTACTGACCATAATATTAAGCAAAAAGAAAAAGCAAGAATTTTATTATTTATTTATTTATTTATTTATTTATTTGAGATAGAGTCTCACTATGTTCCCCAGGCTGGAGTGCAGTTGCATTGATCCTGGCTCACTGCAACCTCTGCCTCTCAGGCTCAAGAGATCCTCCCACCTCAGCCTCCCAAGTAGCTGGGACCACAGGTGGGCACCACCATGCCTGGCAAATTTTTTGTATTTTTGGTAGAGACGGGGTTTCATCATGTTGCTCAGGCTGGTCTCAAACTCCTGAGATAAGCGATTTGCGTTCCTCAGCCTCCCAGACTGTTGGGATTACAGACATGAGCCACAGCACTCGGCAAAGAATTTTAATTTCTACATTTTTTTACCTTTTGGAGGGCCTAAATTCACACACACACATATGACATGTATCCATTCATTTCAGGGGTTAAAACACAGTGGCCTATTATCTCTTACAACTTCATGTGAATCAACAATTATCTCAAGGCTGGACGCAGTGGGTCATGCCTGTAATTCCAGGACTTTGGGAGACCGAGGTGGGCGGATCACTTGAGGCCAGGAGTTCGAGACCAGCATGGCCACCAAGACAAATTCCTGTCTCTACTAAAAATACAAAAATTAGCCAGGCTTGGTGGCACATGCCTGTAATTCCAGCTACTCTGGAGGCTGAGGCGGGAAGATTGTTTGAACCCGGGAGACAGAGGTTACAGTGAGCCAAGATCCCACCACTGCAGTCCAGCCTGGGCAACAGAGTGAGACTCTGTCTCCAAAAAAACCAAACCAAACAAACAAAAAAAAGCCCCACATATTTCAAAAAATTTTATTTTAAGAAAAACAAAAAAACCTACTTTCAATTAAAAAACACAGTGGTTATGTTAATGCATCACCACATTCCTAGAGAAGTATTAAGGCAATGTATGTAAGTAAGTACATATACTCAGGTGCATTGATAATACGATTTATGTATACTTTCAGCAACAACACAAGGGGAGAGAAGATGATTTACTAGCTCTCCTCATAAAGCTACAAACAAAACCACAGATGTAACTCAGGTGGGAGTTTCCTAGTTTATGGCTAACTTTAGCTTATTAACACGACAAATATTATCATTTTCACTGAAAAATTTGACATTTTTTGTTGACTCAGCCAATCCAAGAAGCTTAATTAATATATAGTCCCTGCCCTCAAGAGACTTCCAATATAGTTGGAAGAGACCAGGCAAACACAAAGAAAGCTAACAATGCAATATATACCACAAGGCAGCATATAATTAGTTTTCGAATGAGTGCTAACAGACCTTGAGACCACTGGTTAAAATGTTTATATATCTATATAGATATACATAAATGGTAAATATGTTTTTTCATTTTCAAAAACTTTTTTTCCGTTTTCAAAAACTGTGAACCACCACTAGCAGACACCCTATAACAAGGCCCACTGTTCATTTTTAAATTACACATATAAAATGTTTCATTCAGAAAGCAAACCTGAAGCCCTTCAGCCTCCTCCACACTCACAATTCAAATTATATAGAACCTACGAAGCTGGTTAGAAAGCAGGTAATCTCAGAAAAAAGGGTGGGGGTGACAGCAGAAACAACCAGGGATCCACAAGGTCATACCAAAGGGGTGGTCGGAAAGGAACCTGGGGAAGGGAGAGAAGGCTGGAACGCTTACTTACTTCTTGTTATCAGTGTAGAAGCGTGTTTGGAGCTGAGCCATGGCGAGGAGTACACACGACTTGCCCACGGAAACGTACGGGTTAGCAGACCCACAACACGAAGCTCCTGCCTTTTAAGACTACAAAGAGGCAGCTCAAAATTAGACTGCACAGGTAAGCGAGGAACTGCAGTCTAAGCCTGGACTCTGCCTTCTGCCCTCCGGTCTCCTCTGCAGAAAGCACGAGGTTGCCCTTCTACAGACGCCCAGACCACAAACATCGACTACCTACTCCTAGATTTAGGAGTCATCGATTAGAAAAGGATAGTCGTAAAGGTAGCATGTCCCTTTAATCATTGTTAGAATTCTTCCCATACGTGTGGGCCCGTAATCGATTAGGAAAAAGATCCCACTCAAATCCGCTTTATTAATAAAAAATTTTTTTTCAAGAGGCTGTCCTGAGCCTGGATAGTTCAAACACAGCTCCTAGTTACTACAAGTCTCAGAAAGCATCGCTTCTCAAACGCTGTGACCACCGGGAATTGTAGTCTTTTTTTGCCACGCTGCCCTCAGAGTATCTTATTTGCATTAAATCCTCTAGACTCCAGCCAGATATCAAATTTCCTCTGCCTTCAGCTTTGCTTGACTAACGCCTTGCCAGTTTTTTTTTCTCATACATATTCCCAACCAGAGGCTCACTCGGCAGAGGTTTTTCCTAAATCTCGCCTCTAACTGCGGCTTTTAGAGAGAGGCGAGAGGCGGAAATACAAAGAAAAGAACGGATGAGAGAGGAAACGGTCGACGGTGGCCGATAGTGGTCCTACTGTCGTGGGACTGCTCTGCGCGCTCCCGGTGGGGCGCGCCTGCGCATTATGCTGGTCTCCATGGCGGGGCCTCGGAGCCAAGACGAGGTTGAGTAGACTCGTTTTGAATTTTCTCCCCTCTGCTCCGGCGGACTTCCCATGTCGCCTTGTGGGGCTATCGGCGGCGGCAGGACTGGGGGAGTCAGAGGTCTGGCAGCGCTGTCTGCGCAGACCTACCGGACGCTACCTCCCAACCCCCCGTCTTCCTCCTGCCTCCTCCTCCTCCCGTCACCTCCTGACCCGCCGGAGCTCCGAGCAACTGCCGGCCTCCGCCCCCAGCCGCAGCCGGTCACTGGCGGCGCCTTCCGCGCCAAGCTTGGGGGCCTTTTCGGGGTCCCACATGGCACGGCTTCCGACCCCCGGCCCGGGACGGGGCTCGCAGGCCCCAGAGGGGCAGGCTGGAGAAGGAGGAGGTTAGGTGTCTTCAGGAGGGTTGCTGAGCCCAAGGACGCGCCATCGCCGCGGAGAAGGAGCCGGACCCCTTGGGCGGAGCGCCCAATGTGTGGTCCCTCACGCCGTCCCGCACCTTGCTTTTTAGGGTTCTTTTTCCGCTTTCTGAGCCCTTTTATACCTTACGTTTAGAAGGGGAAAATCATCCTCCCACACCTTCTCCCCGACTTTTTGCCTTTTTTGTCTTGAAGTTACCCAAAGGCCTGTGTATTGTTCTCAATGGTCCCAAGAATTACTCTAATATAGTTGTTTTTCTGAGGGAGGATGGATGGAGATAACTATCCTGATCCCAATGTCACTTTTTAAGGCATTCGCTTCAAGAGACAAGCAGTTTAGAATCAGGCAGAACTGGATTGCAAAATTTATGGGCAGACCGGTATGTGTGCAGGTGAAGACAAAGCTTTCCTTTTTTACGTTGTTTAAAGATGTCCTACTGTAAGGTAGACGCCTAATGTTTAAATGAACGCCTAATATTTAAATAGAGAATGAGAAGAGATTTAAACAATTGGAAGAAAAAATAATTTTTATTAGAGAGCAATGACAATAAAAGTAGGACTCTTTTCAGTTTACTCACCTTTGTAGCTCCGAATCCGGGCCCTGGAACCAGAATTCCTGGGTTCAAATCCTGCCTCTGCTACTTGATTAACCTTTTTGTGCCACACTTTTATCAACTATAAGAATTAACTGCGGCATCGGAAGCACTCAGTATATGTTAATTATTATCATCAAATAGTTACTAATTCCAGTTTAACTAGAAGCTTTGCCAGCGTGAATAGATAAGGGAGAATAGCCACATTGAGTGGAGGAATAAAATTAGAAAACCTCTGTGCTGAAGTCCCAGGAGATCACTTATTAGGCTTGAGCTTGCTAATGGAAGTGGTGATCGTTTAACCATGTGGTTATTGTAGTGGTGAGGATTTACACATTAAATAAGTCTTGGGCATCTGAAGCTTGAAACACTTTCTTAATGAAATCTTGCCGTTGAATCTGTACTTACTGGTAATATGTCATTGGTTAAAATATATTTATAAAATGTATAATTGACCAGCAGGTTAATACATTTTTATCATTAATTTGAAATTTATCAAAATGAGAATATACAGTACCTGACAGTAGAGTTTGATCTTAATTTGATTTGAAAATTATGATCAGGAAAATAACCTGAATATATGCTTAATAATATAAGACTTTCAGTTTGCAAAACAGAAATCCTGACTTGACTAAAATACACTTTTCATTTGACGGAGACCTTTAGTTAAGGAAAGATTAGTTTTAGCATTCAACATAACTAAATACCCAGCATTATGTTTTACGGTTTTTTTCAACAATGATTTATGTGTCGTGTTTGGTGTTGCCTCCAAACAGTACCTAGGATAGCTGATGCTTTTACTTTGAGTCTTTTTCTTTAGTTGGAACATTAATTGCAAGCTTTTGGTTGGTAATCATTTTTTATGGGATGTATTTACTATATGGTCCTCATGGAGACACATTGTTATCTGGAGAACCACAGTTTGGAAAATACATTTCTCTTTATTATAAAGTTTTTTGTTTTATATATATAATGCCACAGTGTGATAGGATACAAAGACTATGCATTAGGGCTTTGGAGTCTTAATTGTTCTTTGTGTTTTTCTTTTTTTCCAATTTGATTATGGATCCAAGCCTAATGCACTCTGTGATCTTAAGCAAGTTTCTTGACAGTTCCAGTTTTCTTATCTACAAACAGAGACAAGAAGATCTACCTCAGAGTCATTGTAAGGATTAAATAAATTTGT', 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/y_h_2twd02q4y_zk07g55xk40000gp/T/ipykernel_20182/901525165.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  wrong_data = torch.load(wrong_file_path)\n"
     ]
    }
   ],
   "source": [
    "wrong_file_path = '3model_result/3model_all_wrong.pt'\n",
    "wrong_data = torch.load(wrong_file_path)\n",
    "\n",
    "print(f\"len is: {len(wrong_data['labels'])}\")\n",
    "print(f\"table head is :{wrong_data.keys()}\")\n",
    "first_row = {key: value[0] for key, value in wrong_data.items()}\n",
    "print(\"First row of data: \", first_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the questions that some models do right and some do wrong\n",
    "#### Of the three models, the sequence in which only two models get it right is compared with the sequence in which only one model gets it right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved one correct data to 3model_result/3model_have_1_correct.pt\n",
      "Saved two correct data to 3model_result/3model_have_2_correct.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/y_h_2twd02q4y_zk07g55xk40000gp/T/ipykernel_72180/940007716.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  results = [torch.load(f) for f in result_files]\n",
      "/var/folders/69/y_h_2twd02q4y_zk07g55xk40000gp/T/ipykernel_72180/940007716.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load(test_file)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "def compare_predictions(result_files, test_file, output_dir=\"3model_result\"):\n",
    "    results = [torch.load(f) for f in result_files]\n",
    "    test_data = torch.load(test_file)\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    num_samples = len(results[0][\"predictions\"])  \n",
    "    num_models = len(results)  \n",
    "    \n",
    "    all_correct_indices = []\n",
    "    all_incorrect_indices = []\n",
    "    one_correct_indices = []\n",
    "    two_correct_indices = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        label = results[0][\"labels\"][i]\n",
    "        predictions = [result[\"predictions\"][i] for result in results]\n",
    "        correct_preds = [pred == label for pred in predictions]\n",
    "        num_correct = sum(correct_preds)\n",
    "\n",
    "        # All the models predicted correctly\n",
    "        if num_correct == num_models:\n",
    "            all_correct_indices.append(i)\n",
    "        \n",
    "        # All the models predicted wrong\n",
    "        elif num_correct == 0:\n",
    "            all_incorrect_indices.append(i)\n",
    "\n",
    "        # Only one model predicted correctly\n",
    "        elif num_correct == 1:\n",
    "            one_correct_indices.append(i)\n",
    "        \n",
    "        # Only two models predicted correctly\n",
    "        elif num_correct == 2:\n",
    "            two_correct_indices.append(i)\n",
    "\n",
    "    # Process samples for which only one model predicts correctly\n",
    "    one_correct_data = {key: [] for key in test_data.keys()}\n",
    "    for idx in one_correct_indices:\n",
    "        for key in test_data.keys():\n",
    "            one_correct_data[key].append(test_data[key][idx])\n",
    "\n",
    "    one_correct_save_path = os.path.join(output_dir, \"3model_have_1_correct.pt\")\n",
    "    torch.save(one_correct_data, one_correct_save_path)\n",
    "    print(f\"Saved one correct data to {one_correct_save_path}\")\n",
    "    \n",
    "    # Process samples where only two models predict correctly\n",
    "    two_correct_data = {key: [] for key in test_data.keys()}\n",
    "    for idx in two_correct_indices:\n",
    "        for key in test_data.keys():\n",
    "            two_correct_data[key].append(test_data[key][idx])\n",
    "\n",
    "    two_correct_save_path = os.path.join(output_dir, \"3model_have_2_correct.pt\")\n",
    "    torch.save(two_correct_data, two_correct_save_path)\n",
    "    print(f\"Saved two correct data to {two_correct_save_path}\")\n",
    "\n",
    "result_files = [\"data/Hyena_602.pt\", \"data/NTv2_240.pt\", \"data/CDgpt_2_8754.pt\"]\n",
    "test_file = \"../test.pt\"\n",
    "compare_predictions(result_files, test_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 看看这些数据里面长啥样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: demo_mouse_enhancers\n",
      "Failed to process dataset demo_mouse_enhancers: Dataset demo_mouse_enhancers not found.\n",
      "Processing dataset: drosophila_enhancers_stark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1D8u3m09CNIv8e4-5rOu5wKuwcLl1eejs\n",
      "From (redirected): https://drive.google.com/uc?id=1D8u3m09CNIv8e4-5rOu5wKuwcLl1eejs&confirm=t&uuid=4888a748-7a36-421a-ba66-94c575d99a8c\n",
      "To: /Users/jacob/.genomic_benchmarks/drosophila_enhancers_stark.zip\n",
      "100%|██████████| 6.34M/6.34M [00:00<00:00, 7.61MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train split for dataset: drosophila_enhancers_stark\n",
      "Saving train split to genomic_benchmark_datasets/train_drosophila_enhancers_stark.pth\n",
      "Loading test split for dataset: drosophila_enhancers_stark\n",
      "Saving test split to genomic_benchmark_datasets/test_drosophila_enhancers_stark.pth\n",
      "Processing dataset: demo_human_or_worm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1JW0-eTB-rJXvFcglqBo3pFZi1kyIWC3X\n",
      "From (redirected): https://drive.google.com/uc?id=1JW0-eTB-rJXvFcglqBo3pFZi1kyIWC3X&confirm=t&uuid=27a5a264-be00-496b-85d1-bdbdc7885777\n",
      "To: /Users/jacob/.genomic_benchmarks/demo_human_or_worm.zip\n",
      "100%|██████████| 28.9M/28.9M [00:04<00:00, 6.47MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train split for dataset: demo_human_or_worm\n",
      "Saving train split to genomic_benchmark_datasets/train_demo_human_or_worm.pth\n",
      "Loading test split for dataset: demo_human_or_worm\n",
      "Saving test split to genomic_benchmark_datasets/test_demo_human_or_worm.pth\n",
      "Processing dataset: demo_coding_vs_intergenomic_seqs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1cpXg0ULuTGF7h1_HTYvc6p8M-ee43t-v\n",
      "From (redirected): https://drive.google.com/uc?id=1cpXg0ULuTGF7h1_HTYvc6p8M-ee43t-v&confirm=t&uuid=7038edf6-96e8-4336-ba81-59c4ce8a485d\n",
      "To: /Users/jacob/.genomic_benchmarks/demo_coding_vs_intergenomic_seqs.zip\n",
      "100%|██████████| 33.9M/33.9M [00:05<00:00, 6.37MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train split for dataset: demo_coding_vs_intergenomic_seqs\n",
      "Saving train split to genomic_benchmark_datasets/train_demo_coding_vs_intergenomic_seqs.pth\n",
      "Loading test split for dataset: demo_coding_vs_intergenomic_seqs\n",
      "Saving test split to genomic_benchmark_datasets/test_demo_coding_vs_intergenomic_seqs.pth\n",
      "Processing dataset: human_enhancers_cohn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=176563cDPQ5Y094WyoSBF02QjoVQhWuCh\n",
      "From (redirected): https://drive.google.com/uc?id=176563cDPQ5Y094WyoSBF02QjoVQhWuCh&confirm=t&uuid=9c1fee05-4c26-4b19-9ed2-f11e5f220d59\n",
      "To: /Users/jacob/.genomic_benchmarks/human_enhancers_cohn.zip\n",
      "100%|██████████| 11.9M/11.9M [00:01<00:00, 9.52MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train split for dataset: human_enhancers_cohn\n",
      "Saving train split to genomic_benchmark_datasets/train_human_enhancers_cohn.pth\n",
      "Loading test split for dataset: human_enhancers_cohn\n",
      "Saving test split to genomic_benchmark_datasets/test_human_enhancers_cohn.pth\n",
      "Processing dataset: human_ocr_ensembl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1y_LInRF2aRXysigpwv_oU3Q67VVxfk18\n",
      "From (redirected): https://drive.google.com/uc?id=1y_LInRF2aRXysigpwv_oU3Q67VVxfk18&confirm=t&uuid=791c0608-c29d-44fe-a4e3-5325e67fff2e\n",
      "To: /Users/jacob/.genomic_benchmarks/human_ocr_ensembl.zip\n",
      "100%|██████████| 59.0M/59.0M [00:06<00:00, 8.77MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train split for dataset: human_ocr_ensembl\n",
      "Saving train split to genomic_benchmark_datasets/train_human_ocr_ensembl.pth\n",
      "Loading test split for dataset: human_ocr_ensembl\n",
      "Saving test split to genomic_benchmark_datasets/test_human_ocr_ensembl.pth\n",
      "Processing dataset: human_enhancers_ensembl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1gZBEV_RGxJE8EON5OObdrp5Tp8JL0Fxb\n",
      "From (redirected): https://drive.google.com/uc?id=1gZBEV_RGxJE8EON5OObdrp5Tp8JL0Fxb&confirm=t&uuid=0eaf8434-8844-4e3f-9f4f-a9892fff7bd3\n",
      "To: /Users/jacob/.genomic_benchmarks/human_enhancers_ensembl.zip\n",
      "100%|██████████| 51.1M/51.1M [00:05<00:00, 9.85MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train split for dataset: human_enhancers_ensembl\n",
      "Saving train split to genomic_benchmark_datasets/train_human_enhancers_ensembl.pth\n",
      "Loading test split for dataset: human_enhancers_ensembl\n",
      "Saving test split to genomic_benchmark_datasets/test_human_enhancers_ensembl.pth\n",
      "Processing dataset: human_nontata_promoters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1VdUg0Zu8yfLS6QesBXwGz1PIQrTW3Ze4\n",
      "From (redirected): https://drive.google.com/uc?id=1VdUg0Zu8yfLS6QesBXwGz1PIQrTW3Ze4&confirm=t&uuid=bb3dd3ef-c910-470c-95d7-b454c7e42005\n",
      "To: /Users/jacob/.genomic_benchmarks/human_nontata_promoters.zip\n",
      "100%|██████████| 11.8M/11.8M [00:01<00:00, 8.03MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train split for dataset: human_nontata_promoters\n",
      "Saving train split to genomic_benchmark_datasets/train_human_nontata_promoters.pth\n",
      "Loading test split for dataset: human_nontata_promoters\n",
      "Saving test split to genomic_benchmark_datasets/test_human_nontata_promoters.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from genomic_benchmarks.loc2seq import download_dataset\n",
    "from genomic_benchmarks.dataset_getters.pytorch_datasets import (\n",
    "    DemoMouseEnhancers,\n",
    "    DrosophilaEnhancersStark,\n",
    "    DemoHumanOrWorm,\n",
    "    DemoCodingVsIntergenomicSeqs,\n",
    "    HumanEnhancersCohn,\n",
    "    HumanOcrEnsembl,\n",
    "    HumanEnsemblRegulatory,\n",
    "    HumanEnhancersEnsembl,\n",
    "    HumanNontataPromoters\n",
    ")\n",
    "\n",
    "# 数据集列表及其对应的类\n",
    "datasets = {\n",
    "    'demo_mouse_enhancers': DemoMouseEnhancers,\n",
    "    'drosophila_enhancers_stark': DrosophilaEnhancersStark,\n",
    "    'demo_human_or_worm': DemoHumanOrWorm,\n",
    "    'demo_coding_vs_intergenomic_seqs': DemoCodingVsIntergenomicSeqs,\n",
    "    'human_enhancers_cohn': HumanEnhancersCohn,\n",
    "    'human_ocr_ensembl': HumanOcrEnsembl,\n",
    "    'human_ensembl_regulatory': HumanEnsemblRegulatory,\n",
    "    'human_enhancers_ensembl': HumanEnhancersEnsembl,\n",
    "    'human_nontata_promoters': HumanNontataPromoters\n",
    "}\n",
    "\n",
    "# 保存路径\n",
    "output_dir = \"genomic_benchmark_datasets\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def save_dataset(dataset_name, dataset_class, version=0):\n",
    "    \"\"\"\n",
    "    下载并保存指定数据集的训练和测试数据到对应文件夹。\n",
    "    \"\"\"\n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "    \n",
    "    # 下载数据集\n",
    "    download_dataset(dataset_name, version=version)\n",
    "    \n",
    "    # 加载训练集和测试集\n",
    "    for split in ['train', 'test']:\n",
    "        print(f\"Loading {split} split for dataset: {dataset_name}\")\n",
    "        dset = dataset_class(split=split, version=version)\n",
    "        \n",
    "        # 提取序列和标签\n",
    "        sequences = [seq for seq, label in dset]\n",
    "        labels = [label for seq, label in dset]\n",
    "        \n",
    "        # 构建保存路径\n",
    "        save_path = os.path.join(\n",
    "            output_dir, \n",
    "            f\"{split}_{dataset_name}.pt\"\n",
    "        )\n",
    "        \n",
    "        # 保存为 .pth 文件\n",
    "        print(f\"Saving {split} split to {save_path}\")\n",
    "        torch.save({\"sequences\": sequences, \"labels\": labels}, save_path)\n",
    "\n",
    "# 遍历数据集，逐一处理\n",
    "for dataset_name, dataset_class in datasets.items():\n",
    "    try:\n",
    "        save_dataset(dataset_name, dataset_class)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process dataset {dataset_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/y_h_2twd02q4y_zk07g55xk40000gp/T/ipykernel_13392/1961917763.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(pth_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted test_human_ocr_ensembl.pth to genomic_benchmark_datasets/test_human_ocr_ensembl.pt\n",
      "Converted test_human_enhancers_ensembl.pth to genomic_benchmark_datasets/test_human_enhancers_ensembl.pt\n",
      "Converted train_human_ocr_ensembl.pth to genomic_benchmark_datasets/train_human_ocr_ensembl.pt\n",
      "Converted test_human_ensembl_regulatory.pth to genomic_benchmark_datasets/test_human_ensembl_regulatory.pt\n",
      "Converted train_human_ensembl_regulatory.pth to genomic_benchmark_datasets/train_human_ensembl_regulatory.pt\n",
      "Converted test_human_enhancers_cohn.pth to genomic_benchmark_datasets/test_human_enhancers_cohn.pt\n",
      "Converted test_human_nontata_promoters.pth to genomic_benchmark_datasets/test_human_nontata_promoters.pt\n",
      "Converted train_human_enhancers_cohn.pth to genomic_benchmark_datasets/train_human_enhancers_cohn.pt\n",
      "Converted train_drosophila_enhancers_stark.pth to genomic_benchmark_datasets/train_drosophila_enhancers_stark.pt\n",
      "Converted train_human_enhancers_ensembl.pth to genomic_benchmark_datasets/train_human_enhancers_ensembl.pt\n",
      "Converted test_drosophila_enhancers_stark.pth to genomic_benchmark_datasets/test_drosophila_enhancers_stark.pt\n",
      "Converted train_human_nontata_promoters.pth to genomic_benchmark_datasets/train_human_nontata_promoters.pt\n",
      "Converted train_demo_human_or_worm.pth to genomic_benchmark_datasets/train_demo_human_or_worm.pt\n",
      "Converted test_demo_coding_vs_intergenomic_seqs.pth to genomic_benchmark_datasets/test_demo_coding_vs_intergenomic_seqs.pt\n",
      "Converted train_demo_coding_vs_intergenomic_seqs.pth to genomic_benchmark_datasets/train_demo_coding_vs_intergenomic_seqs.pt\n",
      "Converted test_demo_human_or_worm.pth to genomic_benchmark_datasets/test_demo_human_or_worm.pt\n",
      "All files converted to .pt format.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# 文件夹路径\n",
    "dataset_dir = \"genomic_benchmark_datasets\"\n",
    "\n",
    "# 获取所有 .pth 文件\n",
    "pth_files = [f for f in os.listdir(dataset_dir) if f.endswith(\".pth\")]\n",
    "\n",
    "# 转换为 .pt 文件\n",
    "for file in pth_files:\n",
    "    pth_path = os.path.join(dataset_dir, file)\n",
    "    pt_path = os.path.join(dataset_dir, file.replace(\".pth\", \".pt\"))\n",
    "    \n",
    "    # 加载并重新保存\n",
    "    data = torch.load(pth_path)\n",
    "    torch.save(data, pt_path)\n",
    "    print(f\"Converted {file} to {pt_path}\")\n",
    "\n",
    "print(\"All files converted to .pt format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: drosophila_enhancers_stark\n",
      "Processing dataset: demo_human_or_worm\n",
      "Processing dataset: demo_coding_vs_intergenomic_seqs\n",
      "Processing dataset: human_enhancers_cohn\n",
      "Processing dataset: human_ocr_ensembl\n",
      "Processing dataset: human_ensembl_regulatory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/y_h_2twd02q4y_zk07g55xk40000gp/T/ipykernel_13392/4024181209.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_data = torch.load(train_file)\n",
      "/var/folders/69/y_h_2twd02q4y_zk07g55xk40000gp/T/ipykernel_13392/4024181209.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load(test_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: human_enhancers_ensembl\n",
      "Processing dataset: human_nontata_promoters\n",
      "                            dataset  train_length  test_length  \\\n",
      "0        drosophila_enhancers_stark          5184         1730   \n",
      "1                demo_human_or_worm         75000        25000   \n",
      "2  demo_coding_vs_intergenomic_seqs         75000        25000   \n",
      "3              human_enhancers_cohn         20843         6948   \n",
      "4                 human_ocr_ensembl        139804        34952   \n",
      "5          human_ensembl_regulatory        231348        57713   \n",
      "6           human_enhancers_ensembl        123872        30970   \n",
      "7           human_nontata_promoters         27097         9034   \n",
      "\n",
      "   train_first_seq_len  test_first_seq_len  train_positive  train_negative  \\\n",
      "0                 2205                2077            2592            2592   \n",
      "1                  200                 200           37500           37500   \n",
      "2                  200                 200           37500           37500   \n",
      "3                  500                 500           10422           10421   \n",
      "4                  146                 330           69902           69902   \n",
      "5                  600                 400           69902           85512   \n",
      "6                  420                 184           61936           61936   \n",
      "7                  251                 251           12355           14742   \n",
      "\n",
      "   test_positive  test_negative  \n",
      "0            865            865  \n",
      "1          12500          12500  \n",
      "2          12500          12500  \n",
      "3           3474           3474  \n",
      "4          17476          17476  \n",
      "5          17476          21378  \n",
      "6          15485          15485  \n",
      "7           4119           4915  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# 数据集路径\n",
    "dataset_dir = \"./genomic_benchmark_datasets\"\n",
    "\n",
    "# 数据集名称列表\n",
    "datasets = [\n",
    "    'drosophila_enhancers_stark',\n",
    "    'demo_human_or_worm',\n",
    "    'demo_coding_vs_intergenomic_seqs',\n",
    "    'human_enhancers_cohn',\n",
    "    'human_ocr_ensembl',\n",
    "    'human_ensembl_regulatory',\n",
    "    'human_enhancers_ensembl',\n",
    "    'human_nontata_promoters'\n",
    "]\n",
    "\n",
    "# 初始化结果列表\n",
    "dataset_summary = []\n",
    "\n",
    "# 遍历数据集\n",
    "for dataset in datasets:\n",
    "    train_file = os.path.join(dataset_dir, f\"train_{dataset}.pt\")\n",
    "    test_file = os.path.join(dataset_dir, f\"test_{dataset}.pt\")\n",
    "\n",
    "    print(f\"Processing dataset: {dataset}\")\n",
    "    \n",
    "    train_length, test_length = 0, 0\n",
    "    train_first_seq_len, test_first_seq_len = 0, 0\n",
    "    train_positive, train_negative = 0, 0\n",
    "    test_positive, test_negative = 0, 0\n",
    "\n",
    "    # 加载训练集\n",
    "    if os.path.exists(train_file):\n",
    "        train_data = torch.load(train_file)\n",
    "        train_length = len(train_data[\"sequences\"])\n",
    "        train_first_seq_len = len(train_data[\"sequences\"][0]) if train_length > 0 else 0\n",
    "        train_positive = sum(1 for label in train_data[\"labels\"] if label == 1)\n",
    "        train_negative = sum(1 for label in train_data[\"labels\"] if label == 0)\n",
    "        del train_data  # 释放内存\n",
    "\n",
    "    # 加载测试集\n",
    "    if os.path.exists(test_file):\n",
    "        test_data = torch.load(test_file)\n",
    "        test_length = len(test_data[\"sequences\"])\n",
    "        test_first_seq_len = len(test_data[\"sequences\"][0]) if test_length > 0 else 0\n",
    "        test_positive = sum(1 for label in test_data[\"labels\"] if label == 1)\n",
    "        test_negative = sum(1 for label in test_data[\"labels\"] if label == 0)\n",
    "        del test_data  # 释放内存\n",
    "\n",
    "    # 保存到结果列表\n",
    "    dataset_summary.append({\n",
    "        \"dataset\": dataset,\n",
    "        \"train_length\": train_length,\n",
    "        \"test_length\": test_length,\n",
    "        \"train_first_seq_len\": train_first_seq_len,\n",
    "        \"test_first_seq_len\": test_first_seq_len,\n",
    "        \"train_positive\": train_positive,\n",
    "        \"train_negative\": train_negative,\n",
    "        \"test_positive\": test_positive,\n",
    "        \"test_negative\": test_negative\n",
    "    })\n",
    "\n",
    "# 转换为 Pandas DataFrame\n",
    "summary_df = pd.DataFrame(dataset_summary)\n",
    "\n",
    "# 打印并保存数据集详情\n",
    "print(summary_df)\n",
    "summary_df.to_csv(\"dataset_summary.csv\", index=False)\n",
    "\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Dataset Summary Information\", dataframe=summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
